
2024-05-26 00:22:09.679428:job config: {'train_data_path': '/data/yll6162/alignntl_dft_3d/tl_dataset/dataset_alignn_matbert-base-cased_chemnlp_prop_Tc_supercon_train.csv', 'val_data_path': '/data/yll6162/alignntl_dft_3d/tl_dataset/dataset_alignn_matbert-base-cased_chemnlp_prop_Tc_supercon_val.csv', 'test_data_path': '/data/yll6162/alignntl_dft_3d/tl_dataset/dataset_alignn_matbert-base-cased_chemnlp_prop_Tc_supercon_test.csv', 'label': 'Tc_supercon', 'input_type': None, 'log_folder': 'log', 'log_file': 'alignn_matbert-base-cased_chemnlp_prop_Tc_supercon.log', 'test_metric': 'mae', 'architecture': '1024Rx4D-512Rx3D-256Rx3D-128Rx3D-64Rx2-32Rx1-1', 'model_seed': 0, 'loss_type': 'mae', 'config_file': 'sample/alignn_matbert-base-cased_chemnlp_prop_Tc_supercon_job.config', 'use_valid': True, 'project': 'MOF', 'regressors': None, 'input_types': None, 'paramsGrid': {'optimizer': 'Adam', 'learning_rate': 0.0001, 'patience': 100, 'dropouts': [0.8, 0.9, 0.7, 0.8], 'EVAL_FREQUENCY': 1000}, 'save_path': 'sample/alignn_matbert-base-cased_chemnlp_prop_Tc_supercon', 'model_path': None, 'last_layer_with_weight': True, 'keras_path': 'model/alignn_matbert-base-cased_chemnlp_prop_Tc_supercon', 'test_size': None, 'val_size': None}
2024-05-26 00:22:09.679490:train data path is  /data/yll6162/alignntl_dft_3d/tl_dataset/dataset_alignn_matbert-base-cased_chemnlp_prop_Tc_supercon_train.csv
2024-05-26 00:22:09.951968:input attribute sets are:  None
2024-05-26 00:22:09.952095:test data path is  /data/yll6162/alignntl_dft_3d/tl_dataset/dataset_alignn_matbert-base-cased_chemnlp_prop_Tc_supercon_test.csv
2024-05-26 00:22:10.046308:val data path is  /data/yll6162/alignntl_dft_3d/tl_dataset/dataset_alignn_matbert-base-cased_chemnlp_prop_Tc_supercon_val.csv
2024-05-26 00:22:10.139916:input attributes are:  Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
       ...
       '758.1', '759.1', '760.1', '761.1', '762.1', '763.1', '764.1', '765.1',
       '766.1', '767.1'],
      dtype='object', length=1536)
2024-05-26 00:22:10.140050:label: Tc_supercon
2024-05-26 00:22:17.970793: train, test, valid sizes:  (842, 1536) (842,) (106, 1536) (106,) (106, 1536) (106,)
2024-05-26 00:22:17.979430:train matrix shape of train_X:  (842, 1536)  train_y:  (842,)
2024-05-26 00:22:17.979520:valid matrix shape of train_X:  (106, 1536)  valid_y:  (106,)
2024-05-26 00:22:17.979559:test matrix shape of valid_X:   (106, 1536)  test_y:  (106,)
2024-05-26 00:22:17.979594:architecture is:  1024Rx4D-512Rx3D-256Rx3D-128Rx3D-64Rx2-32Rx1-1
2024-05-26 00:22:17.979628:learning rate is  0.0001
2024-05-26 00:22:17.979661:model path is  None
2024-05-26 00:22:17.987487:adding fully connected layers with 1024 outputs
2024-05-26 00:22:20.312632:adding residual with fc as the size are different
2024-05-26 00:22:20.320028:adding fully connected layers with 1024 outputs
2024-05-26 00:22:20.325255:adding residual, both sizes are same
2024-05-26 00:22:20.327429:adding fully connected layers with 1024 outputs
2024-05-26 00:22:20.332220:adding residual, both sizes are same
2024-05-26 00:22:20.334152:adding fully connected layers with 1024 outputs
2024-05-26 00:22:20.338972:adding residual, both sizes are same
2024-05-26 00:22:20.341124:adding dropout 0.8
2024-05-26 00:22:20.345805:adding fully connected layers with 512 outputs
2024-05-26 00:22:20.351035:adding residual with fc as the size are different
2024-05-26 00:22:20.357388:adding fully connected layers with 512 outputs
2024-05-26 00:22:20.362391:adding residual, both sizes are same
2024-05-26 00:22:20.365318:adding fully connected layers with 512 outputs
2024-05-26 00:22:20.370043:adding residual, both sizes are same
2024-05-26 00:22:20.371922:adding dropout 0.9
2024-05-26 00:22:20.373572:adding fully connected layers with 256 outputs
2024-05-26 00:22:20.378748:adding residual with fc as the size are different
2024-05-26 00:22:20.384976:adding fully connected layers with 256 outputs
2024-05-26 00:22:20.389963:adding residual, both sizes are same
2024-05-26 00:22:20.396503:adding fully connected layers with 256 outputs
2024-05-26 00:22:20.405392:adding residual, both sizes are same
2024-05-26 00:22:20.407800:adding dropout 0.7
2024-05-26 00:22:20.409904:adding fully connected layers with 128 outputs
2024-05-26 00:22:20.415274:adding residual with fc as the size are different
2024-05-26 00:22:20.421575:adding fully connected layers with 128 outputs
2024-05-26 00:22:20.426423:adding residual, both sizes are same
2024-05-26 00:22:20.428336:adding fully connected layers with 128 outputs
2024-05-26 00:22:20.433223:adding residual, both sizes are same
2024-05-26 00:22:20.435107:adding dropout 0.8
2024-05-26 00:22:20.436727:adding fully connected layers with 64 outputs
2024-05-26 00:22:20.441630:adding residual with fc as the size are different
2024-05-26 00:22:20.448064:adding fully connected layers with 64 outputs
2024-05-26 00:22:20.452854:adding residual, both sizes are same
2024-05-26 00:22:20.454827:adding fully connected layers with 32 outputs
2024-05-26 00:22:20.459826:adding residual with fc as the size are different
2024-05-26 00:22:20.466185:adding final layer with 1 output
2024-05-26 00:22:20.475725:Model: "ElemNet"
2024-05-26 00:22:20.475795:__________________________________________________________________________________________________
2024-05-26 00:22:20.475845:Layer (type)                    Output Shape         Param #     Connected to                     
2024-05-26 00:22:20.475882:==================================================================================================
2024-05-26 00:22:20.476110:elemental_fractions (InputLayer [(None, 1536)]       0                                            
2024-05-26 00:22:20.476158:__________________________________________________________________________________________________
2024-05-26 00:22:20.476342:fc0_0 (Dense)                   (None, 1024)         1573888     elemental_fractions[0][0]        
2024-05-26 00:22:20.476386:__________________________________________________________________________________________________
2024-05-26 00:22:20.476497:fc0_dim_0 (Dense)               (None, 1024)         1573888     elemental_fractions[0][0]        
2024-05-26 00:22:20.476539:__________________________________________________________________________________________________
2024-05-26 00:22:20.476618:tf.__operators__.add (TFOpLambd (None, 1024)         0           fc0_0[0][0]                      
2024-05-26 00:22:20.476659:                                                                 fc0_dim_0[0][0]                  
2024-05-26 00:22:20.476694:__________________________________________________________________________________________________
2024-05-26 00:22:20.476797:fc0_1 (Dense)                   (None, 1024)         1049600     tf.__operators__.add[0][0]       
2024-05-26 00:22:20.476837:__________________________________________________________________________________________________
2024-05-26 00:22:20.476911:tf.__operators__.add_1 (TFOpLam (None, 1024)         0           fc0_1[0][0]                      
2024-05-26 00:22:20.476960:                                                                 tf.__operators__.add[0][0]       
2024-05-26 00:22:20.476994:__________________________________________________________________________________________________
2024-05-26 00:22:20.477098:fc0_2 (Dense)                   (None, 1024)         1049600     tf.__operators__.add_1[0][0]     
2024-05-26 00:22:20.477138:__________________________________________________________________________________________________
2024-05-26 00:22:20.477210:tf.__operators__.add_2 (TFOpLam (None, 1024)         0           fc0_2[0][0]                      
2024-05-26 00:22:20.477252:                                                                 tf.__operators__.add_1[0][0]     
2024-05-26 00:22:20.477286:__________________________________________________________________________________________________
2024-05-26 00:22:20.477383:fc0_3 (Dense)                   (None, 1024)         1049600     tf.__operators__.add_2[0][0]     
2024-05-26 00:22:20.477422:__________________________________________________________________________________________________
2024-05-26 00:22:20.477501:tf.__operators__.add_3 (TFOpLam (None, 1024)         0           fc0_3[0][0]                      
2024-05-26 00:22:20.477542:                                                                 tf.__operators__.add_2[0][0]     
2024-05-26 00:22:20.477576:__________________________________________________________________________________________________
2024-05-26 00:22:20.477647:dropout (Dropout)               (None, 1024)         0           tf.__operators__.add_3[0][0]     
2024-05-26 00:22:20.477683:__________________________________________________________________________________________________
2024-05-26 00:22:20.477783:fc1_0 (Dense)                   (None, 512)          524800      dropout[0][0]                    
2024-05-26 00:22:20.477823:__________________________________________________________________________________________________
2024-05-26 00:22:20.477919:fc1_dim_0 (Dense)               (None, 512)          524800      dropout[0][0]                    
2024-05-26 00:22:20.477964:__________________________________________________________________________________________________
2024-05-26 00:22:20.478036:tf.__operators__.add_4 (TFOpLam (None, 512)          0           fc1_0[0][0]                      
2024-05-26 00:22:20.478077:                                                                 fc1_dim_0[0][0]                  
2024-05-26 00:22:20.478112:__________________________________________________________________________________________________
2024-05-26 00:22:20.478208:fc1_1 (Dense)                   (None, 512)          262656      tf.__operators__.add_4[0][0]     
2024-05-26 00:22:20.478248:__________________________________________________________________________________________________
2024-05-26 00:22:20.478319:tf.__operators__.add_5 (TFOpLam (None, 512)          0           fc1_1[0][0]                      
2024-05-26 00:22:20.478359:                                                                 tf.__operators__.add_4[0][0]     
2024-05-26 00:22:20.478393:__________________________________________________________________________________________________
2024-05-26 00:22:20.478488:fc1_2 (Dense)                   (None, 512)          262656      tf.__operators__.add_5[0][0]     
2024-05-26 00:22:20.478527:__________________________________________________________________________________________________
2024-05-26 00:22:20.478598:tf.__operators__.add_6 (TFOpLam (None, 512)          0           fc1_2[0][0]                      
2024-05-26 00:22:20.478638:                                                                 tf.__operators__.add_5[0][0]     
2024-05-26 00:22:20.478672:__________________________________________________________________________________________________
2024-05-26 00:22:20.478739:dropout_1 (Dropout)             (None, 512)          0           tf.__operators__.add_6[0][0]     
2024-05-26 00:22:20.478775:__________________________________________________________________________________________________
2024-05-26 00:22:20.478870:fc2_0 (Dense)                   (None, 256)          131328      dropout_1[0][0]                  
2024-05-26 00:22:20.478910:__________________________________________________________________________________________________
2024-05-26 00:22:20.479016:fc2_dim_0 (Dense)               (None, 256)          131328      dropout_1[0][0]                  
2024-05-26 00:22:20.479056:__________________________________________________________________________________________________
2024-05-26 00:22:20.479125:tf.__operators__.add_7 (TFOpLam (None, 256)          0           fc2_0[0][0]                      
2024-05-26 00:22:20.479166:                                                                 fc2_dim_0[0][0]                  
2024-05-26 00:22:20.479200:__________________________________________________________________________________________________
2024-05-26 00:22:20.479294:fc2_1 (Dense)                   (None, 256)          65792       tf.__operators__.add_7[0][0]     
2024-05-26 00:22:20.479333:__________________________________________________________________________________________________
2024-05-26 00:22:20.479409:tf.__operators__.add_8 (TFOpLam (None, 256)          0           fc2_1[0][0]                      
2024-05-26 00:22:20.479450:                                                                 tf.__operators__.add_7[0][0]     
2024-05-26 00:22:20.479484:__________________________________________________________________________________________________
2024-05-26 00:22:20.479580:fc2_2 (Dense)                   (None, 256)          65792       tf.__operators__.add_8[0][0]     
2024-05-26 00:22:20.479634:__________________________________________________________________________________________________
2024-05-26 00:22:20.479727:tf.__operators__.add_9 (TFOpLam (None, 256)          0           fc2_2[0][0]                      
2024-05-26 00:22:20.479768:                                                                 tf.__operators__.add_8[0][0]     
2024-05-26 00:22:20.479802:__________________________________________________________________________________________________
2024-05-26 00:22:20.479871:dropout_2 (Dropout)             (None, 256)          0           tf.__operators__.add_9[0][0]     
2024-05-26 00:22:20.479908:__________________________________________________________________________________________________
2024-05-26 00:22:20.480016:fc3_0 (Dense)                   (None, 128)          32896       dropout_2[0][0]                  
2024-05-26 00:22:20.480057:__________________________________________________________________________________________________
2024-05-26 00:22:20.480155:fc3_dim_0 (Dense)               (None, 128)          32896       dropout_2[0][0]                  
2024-05-26 00:22:20.480195:__________________________________________________________________________________________________
2024-05-26 00:22:20.480267:tf.__operators__.add_10 (TFOpLa (None, 128)          0           fc3_0[0][0]                      
2024-05-26 00:22:20.480308:                                                                 fc3_dim_0[0][0]                  
2024-05-26 00:22:20.480343:__________________________________________________________________________________________________
2024-05-26 00:22:20.480439:fc3_1 (Dense)                   (None, 128)          16512       tf.__operators__.add_10[0][0]    
2024-05-26 00:22:20.480480:__________________________________________________________________________________________________
2024-05-26 00:22:20.480553:tf.__operators__.add_11 (TFOpLa (None, 128)          0           fc3_1[0][0]                      
2024-05-26 00:22:20.480594:                                                                 tf.__operators__.add_10[0][0]    
2024-05-26 00:22:20.480630:__________________________________________________________________________________________________
2024-05-26 00:22:20.480727:fc3_2 (Dense)                   (None, 128)          16512       tf.__operators__.add_11[0][0]    
2024-05-26 00:22:20.480768:__________________________________________________________________________________________________
2024-05-26 00:22:20.480840:tf.__operators__.add_12 (TFOpLa (None, 128)          0           fc3_2[0][0]                      
2024-05-26 00:22:20.480891:                                                                 tf.__operators__.add_11[0][0]    
2024-05-26 00:22:20.480930:__________________________________________________________________________________________________
2024-05-26 00:22:20.481042:dropout_3 (Dropout)             (None, 128)          0           tf.__operators__.add_12[0][0]    
2024-05-26 00:22:20.481089:__________________________________________________________________________________________________
2024-05-26 00:22:20.481207:fc4_0 (Dense)                   (None, 64)           8256        dropout_3[0][0]                  
2024-05-26 00:22:20.481248:__________________________________________________________________________________________________
2024-05-26 00:22:20.481348:fc4_dim_0 (Dense)               (None, 64)           8256        dropout_3[0][0]                  
2024-05-26 00:22:20.481393:__________________________________________________________________________________________________
2024-05-26 00:22:20.481468:tf.__operators__.add_13 (TFOpLa (None, 64)           0           fc4_0[0][0]                      
2024-05-26 00:22:20.481510:                                                                 fc4_dim_0[0][0]                  
2024-05-26 00:22:20.481545:__________________________________________________________________________________________________
2024-05-26 00:22:20.481645:fc4_1 (Dense)                   (None, 64)           4160        tf.__operators__.add_13[0][0]    
2024-05-26 00:22:20.481686:__________________________________________________________________________________________________
2024-05-26 00:22:20.481759:tf.__operators__.add_14 (TFOpLa (None, 64)           0           fc4_1[0][0]                      
2024-05-26 00:22:20.481800:                                                                 tf.__operators__.add_13[0][0]    
2024-05-26 00:22:20.481835:__________________________________________________________________________________________________
2024-05-26 00:22:20.481942:fc5_0 (Dense)                   (None, 32)           2080        tf.__operators__.add_14[0][0]    
2024-05-26 00:22:20.481985:__________________________________________________________________________________________________
2024-05-26 00:22:20.482088:fc5_dim_0 (Dense)               (None, 32)           2080        tf.__operators__.add_14[0][0]    
2024-05-26 00:22:20.485810:__________________________________________________________________________________________________
2024-05-26 00:22:20.485902:tf.__operators__.add_15 (TFOpLa (None, 32)           0           fc5_0[0][0]                      
2024-05-26 00:22:20.485955:                                                                 fc5_dim_0[0][0]                  
2024-05-26 00:22:20.485991:__________________________________________________________________________________________________
2024-05-26 00:22:20.486099:fc6 (Dense)                     (None, 1)            33          tf.__operators__.add_15[0][0]    
2024-05-26 00:22:20.486145:==================================================================================================
2024-05-26 00:22:20.486731:Total params: 8,389,409
2024-05-26 00:22:20.486778:Trainable params: 8,389,409
2024-05-26 00:22:20.486815:Non-trainable params: 0
2024-05-26 00:22:20.486849:__________________________________________________________________________________________________
2024-05-26 00:22:20.497255:start training
2024-05-26 00:22:22.117107:2024-05-26 00:22:22.117041: Current epoch: 0, loss: 23.890562057495117, validation loss: 13.416444778442383
2024-05-26 00:22:22.412759:2024-05-26 00:22:22.412703: Current epoch: 1, loss: 7.871215343475342, validation loss: 7.7746405601501465
2024-05-26 00:22:22.705235:2024-05-26 00:22:22.705183: Current epoch: 2, loss: 5.702608108520508, validation loss: 4.282257556915283
2024-05-26 00:22:22.998724:2024-05-26 00:22:22.998671: Current epoch: 3, loss: 4.634711742401123, validation loss: 3.606024980545044
2024-05-26 00:22:23.264293:2024-05-26 00:22:23.264222: Current epoch: 4, loss: 4.008082866668701, validation loss: 4.3290181159973145
2024-05-26 00:22:23.543212:2024-05-26 00:22:23.543172: Current epoch: 5, loss: 4.640532493591309, validation loss: 3.789994716644287
2024-05-26 00:22:23.810731:2024-05-26 00:22:23.810699: Current epoch: 6, loss: 5.173243999481201, validation loss: 6.200711727142334
2024-05-26 00:22:24.109252:2024-05-26 00:22:24.109186: Current epoch: 7, loss: 4.176834583282471, validation loss: 3.0372161865234375
2024-05-26 00:22:24.418487:2024-05-26 00:22:24.418433: Current epoch: 8, loss: 3.0041677951812744, validation loss: 2.2940053939819336
2024-05-26 00:22:24.693064:2024-05-26 00:22:24.693023: Current epoch: 9, loss: 2.974673271179199, validation loss: 2.8204708099365234
2024-05-26 00:22:27.397727:2024-05-26 00:22:27.397680: Current epoch: 10, loss: 3.2963438034057617, validation loss: 3.575110673904419
2024-05-26 00:22:27.670511:2024-05-26 00:22:27.670469: Current epoch: 11, loss: 2.7406976222991943, validation loss: 2.45973801612854
2024-05-26 00:22:27.948428:2024-05-26 00:22:27.948386: Current epoch: 12, loss: 3.1918208599090576, validation loss: 3.0536625385284424
2024-05-26 00:22:28.219160:2024-05-26 00:22:28.219118: Current epoch: 13, loss: 2.552614450454712, validation loss: 2.598745346069336
2024-05-26 00:22:28.489537:2024-05-26 00:22:28.489496: Current epoch: 14, loss: 2.7908787727355957, validation loss: 2.351778030395508
2024-05-26 00:22:28.766532:2024-05-26 00:22:28.766494: Current epoch: 15, loss: 2.192318916320801, validation loss: 2.99822998046875
2024-05-26 00:22:29.045015:2024-05-26 00:22:29.044973: Current epoch: 16, loss: 2.1995770931243896, validation loss: 2.3859755992889404
2024-05-26 00:22:29.331757:2024-05-26 00:22:29.331686: Current epoch: 17, loss: 2.037508964538574, validation loss: 2.6705546379089355
2024-05-26 00:22:29.621900:2024-05-26 00:22:29.621861: Current epoch: 18, loss: 2.0729258060455322, validation loss: 2.3416314125061035
2024-05-26 00:22:29.940823:2024-05-26 00:22:29.940770: Current epoch: 19, loss: 1.8522001504898071, validation loss: 2.0952136516571045
2024-05-26 00:22:32.546173:2024-05-26 00:22:32.546129: Current epoch: 20, loss: 1.8013719320297241, validation loss: 2.191615581512451
2024-05-26 00:22:32.821296:2024-05-26 00:22:32.821257: Current epoch: 21, loss: 1.6776074171066284, validation loss: 2.6284236907958984
2024-05-26 00:22:33.127419:2024-05-26 00:22:33.127357: Current epoch: 22, loss: 1.6784350872039795, validation loss: 2.060920476913452
2024-05-26 00:22:33.441482:2024-05-26 00:22:33.441428: Current epoch: 23, loss: 1.7232584953308105, validation loss: 1.887822151184082
2024-05-26 00:22:33.716659:2024-05-26 00:22:33.716618: Current epoch: 24, loss: 2.297691583633423, validation loss: 2.200226306915283
2024-05-26 00:22:34.002269:2024-05-26 00:22:34.002196: Current epoch: 25, loss: 1.843806505203247, validation loss: 2.1933341026306152
2024-05-26 00:22:34.285215:2024-05-26 00:22:34.285142: Current epoch: 26, loss: 1.681227684020996, validation loss: 2.11548113822937
2024-05-26 00:22:34.572308:2024-05-26 00:22:34.572268: Current epoch: 27, loss: 1.5858192443847656, validation loss: 2.017887830734253
2024-05-26 00:22:34.864861:2024-05-26 00:22:34.864821: Current epoch: 28, loss: 1.6971293687820435, validation loss: 2.1043975353240967
2024-05-26 00:22:35.149511:2024-05-26 00:22:35.149472: Current epoch: 29, loss: 1.9810969829559326, validation loss: 1.950444221496582
2024-05-26 00:22:37.753036:2024-05-26 00:22:37.752995: Current epoch: 30, loss: 1.802248239517212, validation loss: 2.392760992050171
2024-05-26 00:22:38.041332:2024-05-26 00:22:38.041290: Current epoch: 31, loss: 1.6034718751907349, validation loss: 2.026000738143921
2024-05-26 00:22:38.324026:2024-05-26 00:22:38.323984: Current epoch: 32, loss: 1.7374259233474731, validation loss: 2.3003742694854736
2024-05-26 00:22:38.604875:2024-05-26 00:22:38.604836: Current epoch: 33, loss: 1.4342260360717773, validation loss: 2.241306781768799
2024-05-26 00:22:38.888885:2024-05-26 00:22:38.888847: Current epoch: 34, loss: 1.4119898080825806, validation loss: 1.9747378826141357
2024-05-26 00:22:39.164182:2024-05-26 00:22:39.164143: Current epoch: 35, loss: 1.820074200630188, validation loss: 1.9746595621109009
2024-05-26 00:22:39.431386:2024-05-26 00:22:39.431346: Current epoch: 36, loss: 1.4362128973007202, validation loss: 2.1994314193725586
2024-05-26 00:22:39.703009:2024-05-26 00:22:39.702968: Current epoch: 37, loss: 1.58675217628479, validation loss: 2.5903942584991455
2024-05-26 00:22:39.998395:2024-05-26 00:22:39.998358: Current epoch: 38, loss: 1.4166656732559204, validation loss: 2.890872001647949
2024-05-26 00:22:40.298510:2024-05-26 00:22:40.298470: Current epoch: 39, loss: 1.7429523468017578, validation loss: 1.907357096672058
2024-05-26 00:22:42.906377:2024-05-26 00:22:42.906340: Current epoch: 40, loss: 1.265669822692871, validation loss: 2.258934736251831
2024-05-26 00:22:43.185965:2024-05-26 00:22:43.185870: Current epoch: 41, loss: 1.2368634939193726, validation loss: 2.0271730422973633
2024-05-26 00:22:43.469155:2024-05-26 00:22:43.469099: Current epoch: 42, loss: 1.239672064781189, validation loss: 2.1238152980804443
2024-05-26 00:22:43.752044:2024-05-26 00:22:43.751968: Current epoch: 43, loss: 1.3197821378707886, validation loss: 1.9828083515167236
2024-05-26 00:22:44.077750:2024-05-26 00:22:44.077692: Current epoch: 44, loss: 1.4935107231140137, validation loss: 1.8047266006469727
2024-05-26 00:22:44.353222:2024-05-26 00:22:44.353159: Current epoch: 45, loss: 1.4056843519210815, validation loss: 2.233128786087036
2024-05-26 00:22:44.637685:2024-05-26 00:22:44.637629: Current epoch: 46, loss: 1.5166069269180298, validation loss: 2.7721095085144043
2024-05-26 00:22:44.947085:2024-05-26 00:22:44.947047: Current epoch: 47, loss: 1.386283040046692, validation loss: 2.08758282661438
2024-05-26 00:22:45.253441:2024-05-26 00:22:45.253387: Current epoch: 48, loss: 1.1695646047592163, validation loss: 1.6992571353912354
2024-05-26 00:22:45.537736:2024-05-26 00:22:45.537663: Current epoch: 49, loss: 1.283892035484314, validation loss: 1.9984992742538452
2024-05-26 00:22:45.864353:2024-05-26 00:22:45.864285: Current epoch: 50, loss: 1.4130980968475342, validation loss: 1.9487740993499756
2024-05-26 00:22:46.171240:2024-05-26 00:22:46.171199: Current epoch: 51, loss: 1.2511112689971924, validation loss: 2.1402621269226074
2024-05-26 00:22:46.480320:2024-05-26 00:22:46.480268: Current epoch: 52, loss: 1.180603265762329, validation loss: 1.8362634181976318
2024-05-26 00:22:46.782998:2024-05-26 00:22:46.782901: Current epoch: 53, loss: 1.0774224996566772, validation loss: 1.8581010103225708
2024-05-26 00:22:47.090786:2024-05-26 00:22:47.090744: Current epoch: 54, loss: 1.0839701890945435, validation loss: 1.7901500463485718
2024-05-26 00:22:47.439150:2024-05-26 00:22:47.439039: Current epoch: 55, loss: 0.9984586834907532, validation loss: 1.8094085454940796
2024-05-26 00:22:47.731433:2024-05-26 00:22:47.731394: Current epoch: 56, loss: 1.0349960327148438, validation loss: 1.7881115674972534
2024-05-26 00:22:48.025978:2024-05-26 00:22:48.025936: Current epoch: 57, loss: 1.0630682706832886, validation loss: 2.187068223953247
2024-05-26 00:22:48.304632:2024-05-26 00:22:48.304592: Current epoch: 58, loss: 1.2900383472442627, validation loss: 1.9087557792663574
2024-05-26 00:22:48.607884:2024-05-26 00:22:48.607844: Current epoch: 59, loss: 1.0275501012802124, validation loss: 1.966984748840332
2024-05-26 00:22:48.881421:2024-05-26 00:22:48.881381: Current epoch: 60, loss: 1.1319044828414917, validation loss: 2.05635666847229
2024-05-26 00:22:49.153837:2024-05-26 00:22:49.153799: Current epoch: 61, loss: 1.0049439668655396, validation loss: 1.826729655265808
2024-05-26 00:22:49.432050:2024-05-26 00:22:49.431891: Current epoch: 62, loss: 0.9172429442405701, validation loss: 1.8420835733413696
2024-05-26 00:22:49.719887:2024-05-26 00:22:49.719847: Current epoch: 63, loss: 0.9009101390838623, validation loss: 1.8785289525985718
2024-05-26 00:22:49.989757:2024-05-26 00:22:49.989715: Current epoch: 64, loss: 0.8854236602783203, validation loss: 2.0002429485321045
2024-05-26 00:22:50.261169:2024-05-26 00:22:50.261128: Current epoch: 65, loss: 0.8938421010971069, validation loss: 1.8137731552124023
2024-05-26 00:22:50.547387:2024-05-26 00:22:50.547346: Current epoch: 66, loss: 0.9043595790863037, validation loss: 1.9212374687194824
2024-05-26 00:22:50.830263:2024-05-26 00:22:50.830190: Current epoch: 67, loss: 1.1985864639282227, validation loss: 1.8867985010147095
2024-05-26 00:22:51.118566:2024-05-26 00:22:51.118524: Current epoch: 68, loss: 0.8725510835647583, validation loss: 1.8243416547775269
2024-05-26 00:22:51.415108:2024-05-26 00:22:51.415069: Current epoch: 69, loss: 0.993096649646759, validation loss: 1.7670636177062988
2024-05-26 00:22:54.069570:2024-05-26 00:22:54.069528: Current epoch: 70, loss: 1.2809998989105225, validation loss: 2.0567874908447266
2024-05-26 00:22:54.395436:2024-05-26 00:22:54.395335: Current epoch: 71, loss: 1.076783299446106, validation loss: 2.459439277648926
2024-05-26 00:22:54.711752:2024-05-26 00:22:54.711672: Current epoch: 72, loss: 1.0489014387130737, validation loss: 2.0809261798858643
2024-05-26 00:22:55.014182:2024-05-26 00:22:55.014121: Current epoch: 73, loss: 1.2132972478866577, validation loss: 2.1662800312042236
2024-05-26 00:22:55.314162:2024-05-26 00:22:55.314122: Current epoch: 74, loss: 0.9313902258872986, validation loss: 1.8604099750518799
2024-05-26 00:22:55.607575:2024-05-26 00:22:55.607501: Current epoch: 75, loss: 0.9729490876197815, validation loss: 2.0795512199401855
2024-05-26 00:22:55.893746:2024-05-26 00:22:55.893705: Current epoch: 76, loss: 0.9329169392585754, validation loss: 2.8825907707214355
2024-05-26 00:22:56.179261:2024-05-26 00:22:56.179187: Current epoch: 77, loss: 1.2758938074111938, validation loss: 1.7675981521606445
2024-05-26 00:22:56.488875:2024-05-26 00:22:56.488832: Current epoch: 78, loss: 0.9581809043884277, validation loss: 1.954121470451355
2024-05-26 00:22:56.775226:2024-05-26 00:22:56.775186: Current epoch: 79, loss: 1.0411405563354492, validation loss: 1.9084179401397705
2024-05-26 00:22:57.065671:2024-05-26 00:22:57.065608: Current epoch: 80, loss: 1.0762112140655518, validation loss: 1.705433964729309
2024-05-26 00:22:57.345507:2024-05-26 00:22:57.345434: Current epoch: 81, loss: 0.9055083990097046, validation loss: 1.8763997554779053
2024-05-26 00:22:57.661167:2024-05-26 00:22:57.661115: Current epoch: 82, loss: 0.7940087914466858, validation loss: 1.813557505607605
2024-05-26 00:22:57.945321:2024-05-26 00:22:57.945281: Current epoch: 83, loss: 0.7967975735664368, validation loss: 1.7484902143478394
2024-05-26 00:22:58.258338:2024-05-26 00:22:58.258264: Current epoch: 84, loss: 0.8035164475440979, validation loss: 1.8484785556793213
2024-05-26 00:22:58.581837:2024-05-26 00:22:58.581708: Current epoch: 85, loss: 0.892945408821106, validation loss: 1.8672915697097778
2024-05-26 00:22:58.869853:2024-05-26 00:22:58.869780: Current epoch: 86, loss: 0.9128190875053406, validation loss: 1.7297642230987549
2024-05-26 00:22:59.169199:2024-05-26 00:22:59.169152: Current epoch: 87, loss: 0.8583898544311523, validation loss: 1.8550193309783936
2024-05-26 00:22:59.471288:2024-05-26 00:22:59.471216: Current epoch: 88, loss: 0.8845909237861633, validation loss: 1.759870171546936
2024-05-26 00:22:59.785688:2024-05-26 00:22:59.785616: Current epoch: 89, loss: 0.8916832804679871, validation loss: 1.763210654258728
2024-05-26 00:23:02.535108:2024-05-26 00:23:02.535066: Current epoch: 90, loss: 0.934683620929718, validation loss: 1.9143857955932617
2024-05-26 00:23:02.822880:2024-05-26 00:23:02.822808: Current epoch: 91, loss: 0.7929396629333496, validation loss: 1.9492976665496826
2024-05-26 00:23:03.129523:2024-05-26 00:23:03.129484: Current epoch: 92, loss: 0.9134803414344788, validation loss: 1.7541468143463135
2024-05-26 00:23:03.430779:2024-05-26 00:23:03.430743: Current epoch: 93, loss: 0.8149120211601257, validation loss: 2.0685276985168457
2024-05-26 00:23:03.721895:2024-05-26 00:23:03.721822: Current epoch: 94, loss: 1.1898658275604248, validation loss: 2.3095173835754395
2024-05-26 00:23:04.019597:2024-05-26 00:23:04.019525: Current epoch: 95, loss: 1.0058125257492065, validation loss: 1.799980878829956
2024-05-26 00:23:04.308123:2024-05-26 00:23:04.308051: Current epoch: 96, loss: 0.8697552680969238, validation loss: 1.9333372116088867
2024-05-26 00:23:04.600019:2024-05-26 00:23:04.599920: Current epoch: 97, loss: 0.9092680215835571, validation loss: 1.8686673641204834
2024-05-26 00:23:04.903252:2024-05-26 00:23:04.903179: Current epoch: 98, loss: 0.8717535138130188, validation loss: 1.6995837688446045
2024-05-26 00:23:05.205518:2024-05-26 00:23:05.205444: Current epoch: 99, loss: 0.9043504595756531, validation loss: 1.7484283447265625
2024-05-26 00:23:07.821221:2024-05-26 00:23:07.821145: Current epoch: 100, loss: 0.9133170247077942, validation loss: 1.9477224349975586
2024-05-26 00:23:08.107495:2024-05-26 00:23:08.107452: Current epoch: 101, loss: 0.9381707310676575, validation loss: 1.9462614059448242
2024-05-26 00:23:08.390230:2024-05-26 00:23:08.390188: Current epoch: 102, loss: 0.8921353816986084, validation loss: 1.8155839443206787
2024-05-26 00:23:08.670707:2024-05-26 00:23:08.670671: Current epoch: 103, loss: 0.8455641865730286, validation loss: 1.822648286819458
2024-05-26 00:23:08.949738:2024-05-26 00:23:08.949696: Current epoch: 104, loss: 0.7861629128456116, validation loss: 1.9405131340026855
2024-05-26 00:23:09.210424:2024-05-26 00:23:09.210394: Current epoch: 105, loss: 0.860029399394989, validation loss: 1.8366554975509644
2024-05-26 00:23:09.510163:2024-05-26 00:23:09.510089: Current epoch: 106, loss: 0.815867006778717, validation loss: 2.0124409198760986
2024-05-26 00:23:09.792689:2024-05-26 00:23:09.792651: Current epoch: 107, loss: 0.7884982228279114, validation loss: 1.900346279144287
2024-05-26 00:23:10.071762:2024-05-26 00:23:10.071721: Current epoch: 108, loss: 0.720381498336792, validation loss: 1.823777437210083
2024-05-26 00:23:10.356936:2024-05-26 00:23:10.356886: Current epoch: 109, loss: 0.8778870701789856, validation loss: 1.741249918937683
2024-05-26 00:23:12.953898:2024-05-26 00:23:12.953857: Current epoch: 110, loss: 0.8176479935646057, validation loss: 1.7709611654281616
2024-05-26 00:23:13.227087:2024-05-26 00:23:13.227047: Current epoch: 111, loss: 0.7024884819984436, validation loss: 1.7305047512054443
2024-05-26 00:23:13.504669:2024-05-26 00:23:13.504619: Current epoch: 112, loss: 0.7400927543640137, validation loss: 1.8013813495635986
2024-05-26 00:23:13.804024:2024-05-26 00:23:13.803981: Current epoch: 113, loss: 0.9422016143798828, validation loss: 2.0151753425598145
2024-05-26 00:23:14.112395:2024-05-26 00:23:14.112348: Current epoch: 114, loss: 0.8193962574005127, validation loss: 1.7411311864852905
2024-05-26 00:23:14.410457:2024-05-26 00:23:14.410330: Current epoch: 115, loss: 0.9518411755561829, validation loss: 2.0320613384246826
2024-05-26 00:23:14.720950:2024-05-26 00:23:14.720860: Current epoch: 116, loss: 0.8453942537307739, validation loss: 1.9314143657684326
2024-05-26 00:23:15.007282:2024-05-26 00:23:15.007240: Current epoch: 117, loss: 0.786266028881073, validation loss: 1.800248146057129
2024-05-26 00:23:15.285992:2024-05-26 00:23:15.285950: Current epoch: 118, loss: 0.7566737532615662, validation loss: 1.722614049911499
2024-05-26 00:23:15.592159:2024-05-26 00:23:15.592099: Current epoch: 119, loss: 0.6905644536018372, validation loss: 1.8229963779449463
2024-05-26 00:23:15.877375:2024-05-26 00:23:15.877335: Current epoch: 120, loss: 0.8301895260810852, validation loss: 1.805796504020691
2024-05-26 00:23:16.170326:2024-05-26 00:23:16.170255: Current epoch: 121, loss: 0.7019926309585571, validation loss: 1.8011349439620972
2024-05-26 00:23:16.462246:2024-05-26 00:23:16.462207: Current epoch: 122, loss: 0.6908498406410217, validation loss: 1.7922077178955078
2024-05-26 00:23:16.750231:2024-05-26 00:23:16.750169: Current epoch: 123, loss: 0.7801383137702942, validation loss: 1.844430923461914
2024-05-26 00:23:17.036137:2024-05-26 00:23:17.036095: Current epoch: 124, loss: 0.8590794205665588, validation loss: 1.7889246940612793
2024-05-26 00:23:17.337110:2024-05-26 00:23:17.337035: Current epoch: 125, loss: 0.7640341520309448, validation loss: 1.8647607564926147
2024-05-26 00:23:17.619157:2024-05-26 00:23:17.619118: Current epoch: 126, loss: 0.6986200213432312, validation loss: 1.711320400238037
2024-05-26 00:23:17.907066:2024-05-26 00:23:17.907001: Current epoch: 127, loss: 0.7477640509605408, validation loss: 1.8778254985809326
2024-05-26 00:23:18.186888:2024-05-26 00:23:18.186815: Current epoch: 128, loss: 0.8592441082000732, validation loss: 1.8197811841964722
2024-05-26 00:23:18.512406:2024-05-26 00:23:18.512356: Current epoch: 129, loss: 0.7256875038146973, validation loss: 1.8093427419662476
2024-05-26 00:23:18.801866:2024-05-26 00:23:18.801806: Current epoch: 130, loss: 0.7602185606956482, validation loss: 2.034451723098755
2024-05-26 00:23:19.079649:2024-05-26 00:23:19.079582: Current epoch: 131, loss: 0.7355612516403198, validation loss: 1.7851256132125854
2024-05-26 00:23:19.378524:2024-05-26 00:23:19.378452: Current epoch: 132, loss: 0.6544061899185181, validation loss: 1.8949881792068481
2024-05-26 00:23:19.670585:2024-05-26 00:23:19.670547: Current epoch: 133, loss: 0.7228854298591614, validation loss: 1.7736130952835083
2024-05-26 00:23:19.949744:2024-05-26 00:23:19.949702: Current epoch: 134, loss: 0.8435457348823547, validation loss: 1.7037709951400757
2024-05-26 00:23:20.241772:2024-05-26 00:23:20.241732: Current epoch: 135, loss: 0.7171520590782166, validation loss: 1.869089126586914
2024-05-26 00:23:20.528525:2024-05-26 00:23:20.528484: Current epoch: 136, loss: 0.8167158961296082, validation loss: 1.9796512126922607
2024-05-26 00:23:20.827085:2024-05-26 00:23:20.827036: Current epoch: 137, loss: 0.8308796286582947, validation loss: 1.8301945924758911
2024-05-26 00:23:21.135684:2024-05-26 00:23:21.135620: Current epoch: 138, loss: 0.897042989730835, validation loss: 1.7117074728012085
2024-05-26 00:23:21.421875:2024-05-26 00:23:21.421834: Current epoch: 139, loss: 0.8304792046546936, validation loss: 2.0711448192596436
2024-05-26 00:23:21.719633:2024-05-26 00:23:21.719561: Current epoch: 140, loss: 0.8519411683082581, validation loss: 1.8246451616287231
2024-05-26 00:23:22.007237:2024-05-26 00:23:22.007164: Current epoch: 141, loss: 0.7522661089897156, validation loss: 2.0513761043548584
2024-05-26 00:23:22.301564:2024-05-26 00:23:22.301522: Current epoch: 142, loss: 0.8568102121353149, validation loss: 1.8502694368362427
2024-05-26 00:23:22.587699:2024-05-26 00:23:22.587626: Current epoch: 143, loss: 0.8189322352409363, validation loss: 1.8357768058776855
2024-05-26 00:23:22.888574:2024-05-26 00:23:22.888501: Current epoch: 144, loss: 0.8611705303192139, validation loss: 1.796993613243103
2024-05-26 00:23:23.169333:2024-05-26 00:23:23.169293: Current epoch: 145, loss: 0.6733057498931885, validation loss: 1.7851030826568604
2024-05-26 00:23:23.469523:2024-05-26 00:23:23.469427: Current epoch: 146, loss: 0.7785239815711975, validation loss: 1.8769299983978271
2024-05-26 00:23:23.816495:2024-05-26 00:23:23.816392: Current epoch: 147, loss: 0.768259584903717, validation loss: 1.7935534715652466
2024-05-26 00:23:24.167622:2024-05-26 00:23:24.167571: Current epoch: 148, loss: 0.6935391426086426, validation loss: 1.7236274480819702
2024-05-26 00:23:24.237530:the test error is  [2.5820584297180176, 2.5820584297180176]
2024-05-26 00:23:26.618937:saved model to sample/alignn_matbert-base-cased_chemnlp_prop_Tc_supercon
2024-05-26 00:23:27.045074:done