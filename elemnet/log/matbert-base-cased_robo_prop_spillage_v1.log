
2024-05-15 18:03:58.551432:job config: {'train_data_path': '/data/yll6162/alignntl_dft_3d/tl_dataset/dataset_matbert-base-cased_robo_prop_spillage_train.csv', 'val_data_path': '/data/yll6162/alignntl_dft_3d/tl_dataset/dataset_matbert-base-cased_robo_prop_spillage_val.csv', 'test_data_path': '/data/yll6162/alignntl_dft_3d/tl_dataset/dataset_matbert-base-cased_robo_prop_spillage_test.csv', 'label': 'spillage', 'input_type': None, 'log_folder': 'log', 'log_file': 'matbert-base-cased_robo_prop_spillage.log', 'test_metric': 'mae', 'architecture': '1024Rx4D-512Rx3D-256Rx3D-128Rx3D-64Rx2-32Rx1-1', 'model_seed': 0, 'loss_type': 'mae', 'config_file': 'sample/matbert-base-cased_robo_prop_spillage_job.config', 'use_valid': True, 'project': 'MOF', 'regressors': None, 'input_types': None, 'paramsGrid': {'optimizer': 'Adam', 'learning_rate': 0.0001, 'patience': 200, 'dropouts': [0.8, 0.9, 0.7, 0.8], 'EVAL_FREQUENCY': 1000}, 'save_path': 'sample/matbert-base-cased_robo_prop_spillage', 'model_path': None, 'last_layer_with_weight': True, 'keras_path': 'model/matbert-base-cased_robo_prop_spillage', 'test_size': None, 'val_size': None}
2024-05-15 18:03:58.551495:train data path is  /data/yll6162/alignntl_dft_3d/tl_dataset/dataset_matbert-base-cased_robo_prop_spillage_train.csv
2024-05-15 18:03:59.758816:input attribute sets are:  None
2024-05-15 18:03:59.759047:test data path is  /data/yll6162/alignntl_dft_3d/tl_dataset/dataset_matbert-base-cased_robo_prop_spillage_test.csv
2024-05-15 18:03:59.956005:val data path is  /data/yll6162/alignntl_dft_3d/tl_dataset/dataset_matbert-base-cased_robo_prop_spillage_val.csv
2024-05-15 18:04:00.159197:input attributes are:  Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
       ...
       '758', '759', '760', '761', '762', '763', '764', '765', '766', '767'],
      dtype='object', length=768)
2024-05-15 18:04:00.159329:label: spillage
2024-05-15 18:04:05.784825: train, test, valid sizes:  (9041, 768) (9041,) (1131, 768) (1131,) (1131, 768) (1131,)
2024-05-15 18:04:05.836313:train matrix shape of train_X:  (9041, 768)  train_y:  (9041,)
2024-05-15 18:04:05.836431:valid matrix shape of train_X:  (1131, 768)  valid_y:  (1131,)
2024-05-15 18:04:05.836475:test matrix shape of valid_X:   (1131, 768)  test_y:  (1131,)
2024-05-15 18:04:05.836512:architecture is:  1024Rx4D-512Rx3D-256Rx3D-128Rx3D-64Rx2-32Rx1-1
2024-05-15 18:04:05.836550:learning rate is  0.0001
2024-05-15 18:04:05.836585:model path is  None
2024-05-15 18:04:05.849212:adding fully connected layers with 1024 outputs
2024-05-15 18:04:06.464265:adding residual with fc as the size are different
2024-05-15 18:04:06.471262:adding fully connected layers with 1024 outputs
2024-05-15 18:04:06.476406:adding residual, both sizes are same
2024-05-15 18:04:06.478357:adding fully connected layers with 1024 outputs
2024-05-15 18:04:06.483448:adding residual, both sizes are same
2024-05-15 18:04:06.485335:adding fully connected layers with 1024 outputs
2024-05-15 18:04:06.490130:adding residual, both sizes are same
2024-05-15 18:04:06.492102:adding dropout 0.8
2024-05-15 18:04:06.496767:adding fully connected layers with 512 outputs
2024-05-15 18:04:06.502230:adding residual with fc as the size are different
2024-05-15 18:04:06.509310:adding fully connected layers with 512 outputs
2024-05-15 18:04:06.514715:adding residual, both sizes are same
2024-05-15 18:04:06.516650:adding fully connected layers with 512 outputs
2024-05-15 18:04:06.521703:adding residual, both sizes are same
2024-05-15 18:04:06.523615:adding dropout 0.9
2024-05-15 18:04:06.525278:adding fully connected layers with 256 outputs
2024-05-15 18:04:06.530699:adding residual with fc as the size are different
2024-05-15 18:04:06.537218:adding fully connected layers with 256 outputs
2024-05-15 18:04:06.542623:adding residual, both sizes are same
2024-05-15 18:04:06.544552:adding fully connected layers with 256 outputs
2024-05-15 18:04:06.549719:adding residual, both sizes are same
2024-05-15 18:04:06.551640:adding dropout 0.7
2024-05-15 18:04:06.553315:adding fully connected layers with 128 outputs
2024-05-15 18:04:06.559464:adding residual with fc as the size are different
2024-05-15 18:04:06.565932:adding fully connected layers with 128 outputs
2024-05-15 18:04:06.571131:adding residual, both sizes are same
2024-05-15 18:04:06.573010:adding fully connected layers with 128 outputs
2024-05-15 18:04:06.577766:adding residual, both sizes are same
2024-05-15 18:04:06.579999:adding dropout 0.8
2024-05-15 18:04:06.581658:adding fully connected layers with 64 outputs
2024-05-15 18:04:06.593033:adding residual with fc as the size are different
2024-05-15 18:04:06.599345:adding fully connected layers with 64 outputs
2024-05-15 18:04:06.604825:adding residual, both sizes are same
2024-05-15 18:04:06.606713:adding fully connected layers with 32 outputs
2024-05-15 18:04:06.612043:adding residual with fc as the size are different
2024-05-15 18:04:06.618721:adding final layer with 1 output
2024-05-15 18:04:06.632578:Model: "ElemNet"
2024-05-15 18:04:06.632658:__________________________________________________________________________________________________
2024-05-15 18:04:06.632709:Layer (type)                    Output Shape         Param #     Connected to                     
2024-05-15 18:04:06.632746:==================================================================================================
2024-05-15 18:04:06.632976:elemental_fractions (InputLayer [(None, 768)]        0                                            
2024-05-15 18:04:06.633024:__________________________________________________________________________________________________
2024-05-15 18:04:06.633180:fc0_0 (Dense)                   (None, 1024)         787456      elemental_fractions[0][0]        
2024-05-15 18:04:06.633226:__________________________________________________________________________________________________
2024-05-15 18:04:06.633340:fc0_dim_0 (Dense)               (None, 1024)         787456      elemental_fractions[0][0]        
2024-05-15 18:04:06.633384:__________________________________________________________________________________________________
2024-05-15 18:04:06.633468:tf.__operators__.add (TFOpLambd (None, 1024)         0           fc0_0[0][0]                      
2024-05-15 18:04:06.633511:                                                                 fc0_dim_0[0][0]                  
2024-05-15 18:04:06.633547:__________________________________________________________________________________________________
2024-05-15 18:04:06.633654:fc0_1 (Dense)                   (None, 1024)         1049600     tf.__operators__.add[0][0]       
2024-05-15 18:04:06.633698:__________________________________________________________________________________________________
2024-05-15 18:04:06.633775:tf.__operators__.add_1 (TFOpLam (None, 1024)         0           fc0_1[0][0]                      
2024-05-15 18:04:06.633818:                                                                 tf.__operators__.add[0][0]       
2024-05-15 18:04:06.633853:__________________________________________________________________________________________________
2024-05-15 18:04:06.633967:fc0_2 (Dense)                   (None, 1024)         1049600     tf.__operators__.add_1[0][0]     
2024-05-15 18:04:06.634012:__________________________________________________________________________________________________
2024-05-15 18:04:06.634090:tf.__operators__.add_2 (TFOpLam (None, 1024)         0           fc0_2[0][0]                      
2024-05-15 18:04:06.634134:                                                                 tf.__operators__.add_1[0][0]     
2024-05-15 18:04:06.634170:__________________________________________________________________________________________________
2024-05-15 18:04:06.634273:fc0_3 (Dense)                   (None, 1024)         1049600     tf.__operators__.add_2[0][0]     
2024-05-15 18:04:06.634316:__________________________________________________________________________________________________
2024-05-15 18:04:06.634393:tf.__operators__.add_3 (TFOpLam (None, 1024)         0           fc0_3[0][0]                      
2024-05-15 18:04:06.634445:                                                                 tf.__operators__.add_2[0][0]     
2024-05-15 18:04:06.634481:__________________________________________________________________________________________________
2024-05-15 18:04:06.634560:dropout (Dropout)               (None, 1024)         0           tf.__operators__.add_3[0][0]     
2024-05-15 18:04:06.634598:__________________________________________________________________________________________________
2024-05-15 18:04:06.634703:fc1_0 (Dense)                   (None, 512)          524800      dropout[0][0]                    
2024-05-15 18:04:06.634746:__________________________________________________________________________________________________
2024-05-15 18:04:06.634851:fc1_dim_0 (Dense)               (None, 512)          524800      dropout[0][0]                    
2024-05-15 18:04:06.634892:__________________________________________________________________________________________________
2024-05-15 18:04:06.634979:tf.__operators__.add_4 (TFOpLam (None, 512)          0           fc1_0[0][0]                      
2024-05-15 18:04:06.635022:                                                                 fc1_dim_0[0][0]                  
2024-05-15 18:04:06.635058:__________________________________________________________________________________________________
2024-05-15 18:04:06.635163:fc1_1 (Dense)                   (None, 512)          262656      tf.__operators__.add_4[0][0]     
2024-05-15 18:04:06.635206:__________________________________________________________________________________________________
2024-05-15 18:04:06.635283:tf.__operators__.add_5 (TFOpLam (None, 512)          0           fc1_1[0][0]                      
2024-05-15 18:04:06.635326:                                                                 tf.__operators__.add_4[0][0]     
2024-05-15 18:04:06.635363:__________________________________________________________________________________________________
2024-05-15 18:04:06.635466:fc1_2 (Dense)                   (None, 512)          262656      tf.__operators__.add_5[0][0]     
2024-05-15 18:04:06.635508:__________________________________________________________________________________________________
2024-05-15 18:04:06.635585:tf.__operators__.add_6 (TFOpLam (None, 512)          0           fc1_2[0][0]                      
2024-05-15 18:04:06.635629:                                                                 tf.__operators__.add_5[0][0]     
2024-05-15 18:04:06.635665:__________________________________________________________________________________________________
2024-05-15 18:04:06.635739:dropout_1 (Dropout)             (None, 512)          0           tf.__operators__.add_6[0][0]     
2024-05-15 18:04:06.635777:__________________________________________________________________________________________________
2024-05-15 18:04:06.635882:fc2_0 (Dense)                   (None, 256)          131328      dropout_1[0][0]                  
2024-05-15 18:04:06.635929:__________________________________________________________________________________________________
2024-05-15 18:04:06.636036:fc2_dim_0 (Dense)               (None, 256)          131328      dropout_1[0][0]                  
2024-05-15 18:04:06.636078:__________________________________________________________________________________________________
2024-05-15 18:04:06.636154:tf.__operators__.add_7 (TFOpLam (None, 256)          0           fc2_0[0][0]                      
2024-05-15 18:04:06.636199:                                                                 fc2_dim_0[0][0]                  
2024-05-15 18:04:06.636235:__________________________________________________________________________________________________
2024-05-15 18:04:06.636338:fc2_1 (Dense)                   (None, 256)          65792       tf.__operators__.add_7[0][0]     
2024-05-15 18:04:06.636379:__________________________________________________________________________________________________
2024-05-15 18:04:06.636455:tf.__operators__.add_8 (TFOpLam (None, 256)          0           fc2_1[0][0]                      
2024-05-15 18:04:06.636506:                                                                 tf.__operators__.add_7[0][0]     
2024-05-15 18:04:06.636543:__________________________________________________________________________________________________
2024-05-15 18:04:06.636646:fc2_2 (Dense)                   (None, 256)          65792       tf.__operators__.add_8[0][0]     
2024-05-15 18:04:06.636708:__________________________________________________________________________________________________
2024-05-15 18:04:06.636790:tf.__operators__.add_9 (TFOpLam (None, 256)          0           fc2_2[0][0]                      
2024-05-15 18:04:06.636833:                                                                 tf.__operators__.add_8[0][0]     
2024-05-15 18:04:06.636868:__________________________________________________________________________________________________
2024-05-15 18:04:06.636986:dropout_2 (Dropout)             (None, 256)          0           tf.__operators__.add_9[0][0]     
2024-05-15 18:04:06.637037:__________________________________________________________________________________________________
2024-05-15 18:04:06.637158:fc3_0 (Dense)                   (None, 128)          32896       dropout_2[0][0]                  
2024-05-15 18:04:06.637201:__________________________________________________________________________________________________
2024-05-15 18:04:06.637305:fc3_dim_0 (Dense)               (None, 128)          32896       dropout_2[0][0]                  
2024-05-15 18:04:06.637349:__________________________________________________________________________________________________
2024-05-15 18:04:06.637426:tf.__operators__.add_10 (TFOpLa (None, 128)          0           fc3_0[0][0]                      
2024-05-15 18:04:06.637469:                                                                 fc3_dim_0[0][0]                  
2024-05-15 18:04:06.637506:__________________________________________________________________________________________________
2024-05-15 18:04:06.637608:fc3_1 (Dense)                   (None, 128)          16512       tf.__operators__.add_10[0][0]    
2024-05-15 18:04:06.637651:__________________________________________________________________________________________________
2024-05-15 18:04:06.637727:tf.__operators__.add_11 (TFOpLa (None, 128)          0           fc3_1[0][0]                      
2024-05-15 18:04:06.637770:                                                                 tf.__operators__.add_10[0][0]    
2024-05-15 18:04:06.637807:__________________________________________________________________________________________________
2024-05-15 18:04:06.637908:fc3_2 (Dense)                   (None, 128)          16512       tf.__operators__.add_11[0][0]    
2024-05-15 18:04:06.637963:__________________________________________________________________________________________________
2024-05-15 18:04:06.638043:tf.__operators__.add_12 (TFOpLa (None, 128)          0           fc3_2[0][0]                      
2024-05-15 18:04:06.638086:                                                                 tf.__operators__.add_11[0][0]    
2024-05-15 18:04:06.638121:__________________________________________________________________________________________________
2024-05-15 18:04:06.638194:dropout_3 (Dropout)             (None, 128)          0           tf.__operators__.add_12[0][0]    
2024-05-15 18:04:06.638233:__________________________________________________________________________________________________
2024-05-15 18:04:06.638336:fc4_0 (Dense)                   (None, 64)           8256        dropout_3[0][0]                  
2024-05-15 18:04:06.638378:__________________________________________________________________________________________________
2024-05-15 18:04:06.638481:fc4_dim_0 (Dense)               (None, 64)           8256        dropout_3[0][0]                  
2024-05-15 18:04:06.638523:__________________________________________________________________________________________________
2024-05-15 18:04:06.638598:tf.__operators__.add_13 (TFOpLa (None, 64)           0           fc4_0[0][0]                      
2024-05-15 18:04:06.638645:                                                                 fc4_dim_0[0][0]                  
2024-05-15 18:04:06.638682:__________________________________________________________________________________________________
2024-05-15 18:04:06.638784:fc4_1 (Dense)                   (None, 64)           4160        tf.__operators__.add_13[0][0]    
2024-05-15 18:04:06.638826:__________________________________________________________________________________________________
2024-05-15 18:04:06.638901:tf.__operators__.add_14 (TFOpLa (None, 64)           0           fc4_1[0][0]                      
2024-05-15 18:04:06.638956:                                                                 tf.__operators__.add_13[0][0]    
2024-05-15 18:04:06.638994:__________________________________________________________________________________________________
2024-05-15 18:04:06.639097:fc5_0 (Dense)                   (None, 32)           2080        tf.__operators__.add_14[0][0]    
2024-05-15 18:04:06.639139:__________________________________________________________________________________________________
2024-05-15 18:04:06.639240:fc5_dim_0 (Dense)               (None, 32)           2080        tf.__operators__.add_14[0][0]    
2024-05-15 18:04:06.639300:__________________________________________________________________________________________________
2024-05-15 18:04:06.639379:tf.__operators__.add_15 (TFOpLa (None, 32)           0           fc5_0[0][0]                      
2024-05-15 18:04:06.639423:                                                                 fc5_dim_0[0][0]                  
2024-05-15 18:04:06.639459:__________________________________________________________________________________________________
2024-05-15 18:04:06.639560:fc6 (Dense)                     (None, 1)            33          tf.__operators__.add_15[0][0]    
2024-05-15 18:04:06.639602:==================================================================================================
2024-05-15 18:04:06.640243:Total params: 6,816,545
2024-05-15 18:04:06.640299:Trainable params: 6,816,545
2024-05-15 18:04:06.640337:Non-trainable params: 0
2024-05-15 18:04:06.640373:__________________________________________________________________________________________________
2024-05-15 18:04:06.652554:start training
2024-05-15 18:04:11.097456:2024-05-15 18:04:11.097384: Current epoch: 0, loss: 4.388171672821045, validation loss: 0.6155771613121033
2024-05-15 18:04:14.281274:2024-05-15 18:04:14.281219: Current epoch: 1, loss: 0.8241127133369446, validation loss: 0.5526304841041565
2024-05-15 18:04:17.201687:2024-05-15 18:04:17.201564: Current epoch: 2, loss: 0.6224980354309082, validation loss: 0.5550863742828369
2024-05-15 18:04:20.117893:2024-05-15 18:04:20.117813: Current epoch: 3, loss: 0.5673218369483948, validation loss: 0.48915722966194153
2024-05-15 18:04:22.687289:2024-05-15 18:04:22.687228: Current epoch: 4, loss: 0.5242841243743896, validation loss: 0.44478195905685425
2024-05-15 18:04:25.366449:2024-05-15 18:04:25.366331: Current epoch: 5, loss: 0.5283637642860413, validation loss: 0.53119957447052
2024-05-15 18:04:27.864189:2024-05-15 18:04:27.864100: Current epoch: 6, loss: 0.47737056016921997, validation loss: 0.42751458287239075
2024-05-15 18:04:30.441746:2024-05-15 18:04:30.441634: Current epoch: 7, loss: 0.4740968942642212, validation loss: 0.4396096467971802
2024-05-15 18:04:33.260756:2024-05-15 18:04:33.260666: Current epoch: 8, loss: 0.4775610864162445, validation loss: 0.5694465041160583
2024-05-15 18:04:36.166809:2024-05-15 18:04:36.166689: Current epoch: 9, loss: 0.45052969455718994, validation loss: 0.4804264307022095
2024-05-15 18:04:41.412412:2024-05-15 18:04:41.412340: Current epoch: 10, loss: 0.4874706566333771, validation loss: 0.5037258863449097
2024-05-15 18:04:44.115709:2024-05-15 18:04:44.115631: Current epoch: 11, loss: 0.43873274326324463, validation loss: 0.39776313304901123
2024-05-15 18:04:46.879818:2024-05-15 18:04:46.879761: Current epoch: 12, loss: 0.4398325979709625, validation loss: 0.4255426526069641
2024-05-15 18:04:49.432106:2024-05-15 18:04:49.432039: Current epoch: 13, loss: 0.43200740218162537, validation loss: 0.40522071719169617
2024-05-15 18:04:51.967523:2024-05-15 18:04:51.967453: Current epoch: 14, loss: 0.4258941411972046, validation loss: 0.4202365279197693
2024-05-15 18:04:54.703798:2024-05-15 18:04:54.703650: Current epoch: 15, loss: 0.43449997901916504, validation loss: 0.40616971254348755
2024-05-15 18:04:57.448405:2024-05-15 18:04:57.448297: Current epoch: 16, loss: 0.418285608291626, validation loss: 0.44485166668891907
2024-05-15 18:05:00.195677:2024-05-15 18:05:00.195545: Current epoch: 17, loss: 0.4100216329097748, validation loss: 0.39902669191360474
2024-05-15 18:05:02.792466:2024-05-15 18:05:02.792344: Current epoch: 18, loss: 0.4363977015018463, validation loss: 0.6487034559249878
2024-05-15 18:05:05.404307:2024-05-15 18:05:05.404252: Current epoch: 19, loss: 0.42974191904067993, validation loss: 0.41845792531967163
2024-05-15 18:05:10.778536:2024-05-15 18:05:10.778457: Current epoch: 20, loss: 0.4128764867782593, validation loss: 0.3980870544910431
2024-05-15 18:05:13.567000:2024-05-15 18:05:13.566879: Current epoch: 21, loss: 0.4245246946811676, validation loss: 0.40791189670562744
2024-05-15 18:05:16.412744:2024-05-15 18:05:16.412646: Current epoch: 22, loss: 0.40881845355033875, validation loss: 0.5026176571846008
2024-05-15 18:05:19.124813:2024-05-15 18:05:19.124729: Current epoch: 23, loss: 0.39554649591445923, validation loss: 0.4219816327095032
2024-05-15 18:05:21.838213:2024-05-15 18:05:21.838132: Current epoch: 24, loss: 0.40468916296958923, validation loss: 0.44914448261260986
2024-05-15 18:05:24.488146:2024-05-15 18:05:24.488107: Current epoch: 25, loss: 0.39760443568229675, validation loss: 0.4621979892253876
2024-05-15 18:05:27.404625:2024-05-15 18:05:27.404547: Current epoch: 26, loss: 0.3888413608074188, validation loss: 0.45114514231681824
2024-05-15 18:05:30.174228:2024-05-15 18:05:30.174165: Current epoch: 27, loss: 0.38486868143081665, validation loss: 0.390953004360199
2024-05-15 18:05:32.898343:2024-05-15 18:05:32.898222: Current epoch: 28, loss: 0.38180816173553467, validation loss: 0.39160406589508057
2024-05-15 18:05:35.118668:2024-05-15 18:05:35.118619: Current epoch: 29, loss: 0.38103508949279785, validation loss: 0.43116089701652527
2024-05-15 18:05:37.858611:2024-05-15 18:05:37.858520: Current epoch: 30, loss: 0.39368316531181335, validation loss: 0.40832993388175964
2024-05-15 18:05:40.360695:2024-05-15 18:05:40.360612: Current epoch: 31, loss: 0.38046637177467346, validation loss: 0.3982907235622406
2024-05-15 18:05:42.983202:2024-05-15 18:05:42.983151: Current epoch: 32, loss: 0.37433329224586487, validation loss: 0.37052446603775024
2024-05-15 18:05:45.649122:2024-05-15 18:05:45.649047: Current epoch: 33, loss: 0.3749845325946808, validation loss: 0.4083133637905121
2024-05-15 18:05:48.298339:2024-05-15 18:05:48.298194: Current epoch: 34, loss: 0.3631238043308258, validation loss: 0.4658615291118622
2024-05-15 18:05:50.912206:2024-05-15 18:05:50.912146: Current epoch: 35, loss: 0.3678804039955139, validation loss: 0.37942034006118774
2024-05-15 18:05:53.317431:2024-05-15 18:05:53.317379: Current epoch: 36, loss: 0.35410964488983154, validation loss: 0.41668829321861267
2024-05-15 18:05:55.611082:2024-05-15 18:05:55.611012: Current epoch: 37, loss: 0.36328426003456116, validation loss: 0.37391915917396545
2024-05-15 18:05:58.019884:2024-05-15 18:05:58.019785: Current epoch: 38, loss: 0.3599506616592407, validation loss: 0.37282484769821167
2024-05-15 18:06:00.722829:2024-05-15 18:06:00.722681: Current epoch: 39, loss: 0.340593159198761, validation loss: 0.38563457131385803
2024-05-15 18:06:06.065535:2024-05-15 18:06:06.065467: Current epoch: 40, loss: 0.33892497420310974, validation loss: 0.36209169030189514
2024-05-15 18:06:08.757828:2024-05-15 18:06:08.757715: Current epoch: 41, loss: 0.33734771609306335, validation loss: 0.3847770094871521
2024-05-15 18:06:11.427621:2024-05-15 18:06:11.427524: Current epoch: 42, loss: 0.3408154845237732, validation loss: 0.3712826371192932
2024-05-15 18:06:14.076848:2024-05-15 18:06:14.076705: Current epoch: 43, loss: 0.33756783604621887, validation loss: 0.390804260969162
2024-05-15 18:06:16.681111:2024-05-15 18:06:16.681056: Current epoch: 44, loss: 0.3294263780117035, validation loss: 0.36032089591026306
2024-05-15 18:06:19.081615:2024-05-15 18:06:19.081527: Current epoch: 45, loss: 0.3253267705440521, validation loss: 0.36432313919067383
2024-05-15 18:06:21.609698:2024-05-15 18:06:21.609620: Current epoch: 46, loss: 0.32581841945648193, validation loss: 0.3763006925582886
2024-05-15 18:06:24.051564:2024-05-15 18:06:24.051517: Current epoch: 47, loss: 0.3202173113822937, validation loss: 0.4237217605113983
2024-05-15 18:06:26.812565:2024-05-15 18:06:26.812476: Current epoch: 48, loss: 0.3221297860145569, validation loss: 0.4090363681316376
2024-05-15 18:06:28.784618:2024-05-15 18:06:28.784484: Current epoch: 49, loss: 0.3143959641456604, validation loss: 0.36600440740585327
2024-05-15 18:06:34.153746:2024-05-15 18:06:34.153647: Current epoch: 50, loss: 0.3043268620967865, validation loss: 0.36991244554519653
2024-05-15 18:06:36.782014:2024-05-15 18:06:36.781896: Current epoch: 51, loss: 0.31217122077941895, validation loss: 0.37486711144447327
2024-05-15 18:06:39.425984:2024-05-15 18:06:39.425915: Current epoch: 52, loss: 0.3021506369113922, validation loss: 0.37725988030433655
2024-05-15 18:06:42.062233:2024-05-15 18:06:42.062139: Current epoch: 53, loss: 0.2987251281738281, validation loss: 0.37154442071914673
2024-05-15 18:06:44.363794:2024-05-15 18:06:44.363756: Current epoch: 54, loss: 0.28912854194641113, validation loss: 0.3637567162513733
2024-05-15 18:06:46.837018:2024-05-15 18:06:46.836968: Current epoch: 55, loss: 0.29185470938682556, validation loss: 0.38955408334732056
2024-05-15 18:06:49.470874:2024-05-15 18:06:49.470809: Current epoch: 56, loss: 0.2871681749820709, validation loss: 0.36375048756599426
2024-05-15 18:06:52.058940:2024-05-15 18:06:52.058804: Current epoch: 57, loss: 0.29041019082069397, validation loss: 0.397157222032547
2024-05-15 18:06:54.847111:2024-05-15 18:06:54.847014: Current epoch: 58, loss: 0.28571441769599915, validation loss: 0.3595901131629944
2024-05-15 18:06:57.553594:2024-05-15 18:06:57.553543: Current epoch: 59, loss: 0.28095996379852295, validation loss: 0.364757776260376
2024-05-15 18:07:02.772737:2024-05-15 18:07:02.772590: Current epoch: 60, loss: 0.27686047554016113, validation loss: 0.3902173340320587
2024-05-15 18:07:05.035634:2024-05-15 18:07:05.035542: Current epoch: 61, loss: 0.2750578820705414, validation loss: 0.3771793842315674
2024-05-15 18:07:07.328757:2024-05-15 18:07:07.328688: Current epoch: 62, loss: 0.27088335156440735, validation loss: 0.353849858045578
2024-05-15 18:07:09.862794:2024-05-15 18:07:09.862636: Current epoch: 63, loss: 0.2571651041507721, validation loss: 0.39347246289253235
2024-05-15 18:07:12.446240:2024-05-15 18:07:12.446054: Current epoch: 64, loss: 0.2557516098022461, validation loss: 0.3786223232746124
2024-05-15 18:07:15.142066:2024-05-15 18:07:15.141905: Current epoch: 65, loss: 0.2630673050880432, validation loss: 0.37960878014564514
2024-05-15 18:07:17.678910:2024-05-15 18:07:17.678807: Current epoch: 66, loss: 0.26160094141960144, validation loss: 0.35811468958854675
2024-05-15 18:07:20.255638:2024-05-15 18:07:20.255600: Current epoch: 67, loss: 0.24861246347427368, validation loss: 0.35582873225212097
2024-05-15 18:07:22.563826:2024-05-15 18:07:22.563790: Current epoch: 68, loss: 0.24626334011554718, validation loss: 0.3542965054512024
2024-05-15 18:07:24.857593:2024-05-15 18:07:24.857521: Current epoch: 69, loss: 0.2490416169166565, validation loss: 0.37382838129997253
2024-05-15 18:07:27.522533:2024-05-15 18:07:27.522416: Current epoch: 70, loss: 0.24694080650806427, validation loss: 0.3677763044834137
2024-05-15 18:07:30.040950:2024-05-15 18:07:30.040794: Current epoch: 71, loss: 0.24093177914619446, validation loss: 0.35907337069511414
2024-05-15 18:07:32.931169:2024-05-15 18:07:32.931034: Current epoch: 72, loss: 0.24067942798137665, validation loss: 0.3698054254055023
2024-05-15 18:07:35.518262:2024-05-15 18:07:35.518211: Current epoch: 73, loss: 0.24330566823482513, validation loss: 0.3519873321056366
2024-05-15 18:07:38.104906:2024-05-15 18:07:38.104857: Current epoch: 74, loss: 0.23032228648662567, validation loss: 0.3513828217983246
2024-05-15 18:07:40.270286:2024-05-15 18:07:40.270214: Current epoch: 75, loss: 0.2275010198354721, validation loss: 0.3592492341995239
2024-05-15 18:07:42.766165:2024-05-15 18:07:42.766057: Current epoch: 76, loss: 0.22449876368045807, validation loss: 0.3593688905239105
2024-05-15 18:07:45.244333:2024-05-15 18:07:45.244210: Current epoch: 77, loss: 0.22304017841815948, validation loss: 0.3714946210384369
2024-05-15 18:07:47.901026:2024-05-15 18:07:47.900916: Current epoch: 78, loss: 0.22051498293876648, validation loss: 0.36126989126205444
2024-05-15 18:07:50.449420:2024-05-15 18:07:50.449332: Current epoch: 79, loss: 0.21958409249782562, validation loss: 0.3595767915248871
2024-05-15 18:07:55.803517:2024-05-15 18:07:55.803384: Current epoch: 80, loss: 0.21477630734443665, validation loss: 0.35260897874832153
2024-05-15 18:07:58.451949:2024-05-15 18:07:58.451874: Current epoch: 81, loss: 0.2085431069135666, validation loss: 0.349391907453537
2024-05-15 18:08:00.856411:2024-05-15 18:08:00.856283: Current epoch: 82, loss: 0.20507417619228363, validation loss: 0.36576777696609497
2024-05-15 18:08:03.405515:2024-05-15 18:08:03.405364: Current epoch: 83, loss: 0.2080707997083664, validation loss: 0.3576771020889282
2024-05-15 18:08:06.001833:2024-05-15 18:08:06.001682: Current epoch: 84, loss: 0.20169411599636078, validation loss: 0.3970712423324585
2024-05-15 18:08:08.533007:2024-05-15 18:08:08.532867: Current epoch: 85, loss: 0.20758415758609772, validation loss: 0.39640113711357117
2024-05-15 18:08:11.219756:2024-05-15 18:08:11.219635: Current epoch: 86, loss: 0.19806517660617828, validation loss: 0.362667053937912
2024-05-15 18:08:13.860095:2024-05-15 18:08:13.859949: Current epoch: 87, loss: 0.19866633415222168, validation loss: 0.38496437668800354
2024-05-15 18:08:16.571947:2024-05-15 18:08:16.571880: Current epoch: 88, loss: 0.19313444197177887, validation loss: 0.384357213973999
2024-05-15 18:08:19.332384:2024-05-15 18:08:19.332240: Current epoch: 89, loss: 0.1904795616865158, validation loss: 0.35996437072753906
2024-05-15 18:08:21.891156:2024-05-15 18:08:21.891071: Current epoch: 90, loss: 0.18534384667873383, validation loss: 0.35461631417274475
2024-05-15 18:08:24.402265:2024-05-15 18:08:24.402195: Current epoch: 91, loss: 0.19009113311767578, validation loss: 0.35313642024993896
2024-05-15 18:08:27.109023:2024-05-15 18:08:27.108863: Current epoch: 92, loss: 0.1873643845319748, validation loss: 0.3817308247089386
2024-05-15 18:08:29.475809:2024-05-15 18:08:29.475686: Current epoch: 93, loss: 0.181695818901062, validation loss: 0.35879069566726685
2024-05-15 18:08:31.715589:2024-05-15 18:08:31.715449: Current epoch: 94, loss: 0.17304424941539764, validation loss: 0.3661106824874878
2024-05-15 18:08:34.078134:2024-05-15 18:08:34.078032: Current epoch: 95, loss: 0.1735304296016693, validation loss: 0.3551216423511505
2024-05-15 18:08:36.213804:2024-05-15 18:08:36.213761: Current epoch: 96, loss: 0.17383629083633423, validation loss: 0.3551046550273895
2024-05-15 18:08:38.525316:2024-05-15 18:08:38.525201: Current epoch: 97, loss: 0.17702116072177887, validation loss: 0.3576471209526062
2024-05-15 18:08:40.925734:2024-05-15 18:08:40.925576: Current epoch: 98, loss: 0.1677948385477066, validation loss: 0.3647528886795044
2024-05-15 18:08:43.331879:2024-05-15 18:08:43.331809: Current epoch: 99, loss: 0.17042744159698486, validation loss: 0.34726274013519287
2024-05-15 18:08:48.547007:2024-05-15 18:08:48.546846: Current epoch: 100, loss: 0.16680428385734558, validation loss: 0.3565576374530792
2024-05-15 18:08:51.251439:2024-05-15 18:08:51.251327: Current epoch: 101, loss: 0.1637386977672577, validation loss: 0.3684377074241638
2024-05-15 18:08:53.741421:2024-05-15 18:08:53.741330: Current epoch: 102, loss: 0.1612103134393692, validation loss: 0.3673134744167328
2024-05-15 18:08:56.500750:2024-05-15 18:08:56.500684: Current epoch: 103, loss: 0.16259264945983887, validation loss: 0.3627403974533081