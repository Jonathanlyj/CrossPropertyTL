
2024-02-15 00:40:23.313746:job config: {'train_data_path': '/data/yll6162/alignntl_dft_3d/tl_dataset/dataset_bert-base-uncased_robo_prop_Tc_supercon.csv', 'val_data_path': None, 'test_data_path': None, 'label': 'Tc_supercon', 'input_type': None, 'log_folder': 'log', 'log_file': 'bert-base-uncased_robo_prop_Tc_supercon.log', 'test_metric': 'mae', 'architecture': '1024Rx4D-512Rx3D-256Rx3D-128Rx3D-64Rx2-32Rx1-1', 'model_seed': 0, 'loss_type': 'mae', 'config_file': 'sample/bert-base-uncased_robo_prop_Tc_supercon_job.config', 'use_valid': True, 'project': 'MOF', 'regressors': None, 'input_types': None, 'paramsGrid': {'optimizer': 'Adam', 'learning_rate': 0.0001, 'patience': 100, 'dropouts': [0.8, 0.9, 0.7, 0.8], 'EVAL_FREQUENCY': 1000}, 'save_path': 'sample/bert-base-uncased_robo_prop_Tc_supercon', 'model_path': None, 'last_layer_with_weight': True, 'keras_path': 'model/bert-base-uncased_robo_prop_Tc_supercon', 'test_size': 0.1, 'val_size': 0.1}
2024-02-15 00:40:23.313817:train data path is  /data/yll6162/alignntl_dft_3d/tl_dataset/dataset_bert-base-uncased_robo_prop_Tc_supercon.csv
2024-02-15 00:40:23.452703:input attribute sets are:  None
2024-02-15 00:40:23.452816:splitting data into with test ratio= 0.1
2024-02-15 00:40:24.256717:input attributes are:  Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
       ...
       '758', '759', '760', '761', '762', '763', '764', '765', '766', '767'],
      dtype='object', length=768)
2024-02-15 00:40:24.256920:label: Tc_supercon
2024-02-15 00:40:28.350607: train, test, valid sizes:  (704, 768) (704,) (89, 768) (89,) (89, 768) (89,)
2024-02-15 00:40:28.354128:train matrix shape of train_X:  (704, 768)  train_y:  (704,)
2024-02-15 00:40:28.354210:valid matrix shape of train_X:  (89, 768)  valid_y:  (89,)
2024-02-15 00:40:28.354253:test matrix shape of valid_X:   (89, 768)  test_y:  (89,)
2024-02-15 00:40:28.354291:architecture is:  1024Rx4D-512Rx3D-256Rx3D-128Rx3D-64Rx2-32Rx1-1
2024-02-15 00:40:28.354329:learning rate is  0.0001
2024-02-15 00:40:28.354364:model path is  None
2024-02-15 00:40:28.362128:adding fully connected layers with 1024 outputs
2024-02-15 00:40:30.787013:adding residual with fc as the size are different
2024-02-15 00:40:30.793884:adding fully connected layers with 1024 outputs
2024-02-15 00:40:30.798861:adding residual, both sizes are same
2024-02-15 00:40:30.800766:adding fully connected layers with 1024 outputs
2024-02-15 00:40:30.805403:adding residual, both sizes are same
2024-02-15 00:40:30.807244:adding fully connected layers with 1024 outputs
2024-02-15 00:40:30.811714:adding residual, both sizes are same
2024-02-15 00:40:30.813637:adding dropout 0.8
2024-02-15 00:40:30.817844:adding fully connected layers with 512 outputs
2024-02-15 00:40:30.822807:adding residual with fc as the size are different
2024-02-15 00:40:30.828961:adding fully connected layers with 512 outputs
2024-02-15 00:40:30.833843:adding residual, both sizes are same
2024-02-15 00:40:30.835759:adding fully connected layers with 512 outputs
2024-02-15 00:40:30.840483:adding residual, both sizes are same
2024-02-15 00:40:30.842343:adding dropout 0.9
2024-02-15 00:40:30.843912:adding fully connected layers with 256 outputs
2024-02-15 00:40:30.848835:adding residual with fc as the size are different
2024-02-15 00:40:30.855167:adding fully connected layers with 256 outputs
2024-02-15 00:40:30.859931:adding residual, both sizes are same
2024-02-15 00:40:30.861689:adding fully connected layers with 256 outputs
2024-02-15 00:40:30.866500:adding residual, both sizes are same
2024-02-15 00:40:30.868305:adding dropout 0.7
2024-02-15 00:40:30.869884:adding fully connected layers with 128 outputs
2024-02-15 00:40:30.874861:adding residual with fc as the size are different
2024-02-15 00:40:30.890428:adding fully connected layers with 128 outputs
2024-02-15 00:40:30.904427:adding residual, both sizes are same
2024-02-15 00:40:30.907490:adding fully connected layers with 128 outputs
2024-02-15 00:40:30.913435:adding residual, both sizes are same
2024-02-15 00:40:30.916006:adding dropout 0.8
2024-02-15 00:40:30.917574:adding fully connected layers with 64 outputs
2024-02-15 00:40:30.922419:adding residual with fc as the size are different
2024-02-15 00:40:30.928539:adding fully connected layers with 64 outputs
2024-02-15 00:40:30.933591:adding residual, both sizes are same
2024-02-15 00:40:30.935446:adding fully connected layers with 32 outputs
2024-02-15 00:40:30.940310:adding residual with fc as the size are different
2024-02-15 00:40:30.946625:adding final layer with 1 output
2024-02-15 00:40:30.956102:Model: "ElemNet"
2024-02-15 00:40:30.956172:__________________________________________________________________________________________________
2024-02-15 00:40:30.956219:Layer (type)                    Output Shape         Param #     Connected to                     
2024-02-15 00:40:30.956255:==================================================================================================
2024-02-15 00:40:30.956456:elemental_fractions (InputLayer [(None, 768)]        0                                            
2024-02-15 00:40:30.956502:__________________________________________________________________________________________________
2024-02-15 00:40:30.956650:fc0_0 (Dense)                   (None, 1024)         787456      elemental_fractions[0][0]        
2024-02-15 00:40:30.956696:__________________________________________________________________________________________________
2024-02-15 00:40:30.956820:fc0_dim_0 (Dense)               (None, 1024)         787456      elemental_fractions[0][0]        
2024-02-15 00:40:30.956865:__________________________________________________________________________________________________
2024-02-15 00:40:30.956947:tf.__operators__.add (TFOpLambd (None, 1024)         0           fc0_0[0][0]                      
2024-02-15 00:40:30.956990:                                                                 fc0_dim_0[0][0]                  
2024-02-15 00:40:30.957026:__________________________________________________________________________________________________
2024-02-15 00:40:30.957129:fc0_1 (Dense)                   (None, 1024)         1049600     tf.__operators__.add[0][0]       
2024-02-15 00:40:30.957172:__________________________________________________________________________________________________
2024-02-15 00:40:30.957248:tf.__operators__.add_1 (TFOpLam (None, 1024)         0           fc0_1[0][0]                      
2024-02-15 00:40:30.957290:                                                                 tf.__operators__.add[0][0]       
2024-02-15 00:40:30.957324:__________________________________________________________________________________________________
2024-02-15 00:40:30.957434:fc0_2 (Dense)                   (None, 1024)         1049600     tf.__operators__.add_1[0][0]     
2024-02-15 00:40:30.957477:__________________________________________________________________________________________________
2024-02-15 00:40:30.957555:tf.__operators__.add_2 (TFOpLam (None, 1024)         0           fc0_2[0][0]                      
2024-02-15 00:40:30.957597:                                                                 tf.__operators__.add_1[0][0]     
2024-02-15 00:40:30.957632:__________________________________________________________________________________________________
2024-02-15 00:40:30.957750:fc0_3 (Dense)                   (None, 1024)         1049600     tf.__operators__.add_2[0][0]     
2024-02-15 00:40:30.957794:__________________________________________________________________________________________________
2024-02-15 00:40:30.957871:tf.__operators__.add_3 (TFOpLam (None, 1024)         0           fc0_3[0][0]                      
2024-02-15 00:40:30.957915:                                                                 tf.__operators__.add_2[0][0]     
2024-02-15 00:40:30.957951:__________________________________________________________________________________________________
2024-02-15 00:40:30.958027:dropout (Dropout)               (None, 1024)         0           tf.__operators__.add_3[0][0]     
2024-02-15 00:40:30.958068:__________________________________________________________________________________________________
2024-02-15 00:40:30.958183:fc1_0 (Dense)                   (None, 512)          524800      dropout[0][0]                    
2024-02-15 00:40:30.958226:__________________________________________________________________________________________________
2024-02-15 00:40:30.958331:fc1_dim_0 (Dense)               (None, 512)          524800      dropout[0][0]                    
2024-02-15 00:40:30.958374:__________________________________________________________________________________________________
2024-02-15 00:40:30.958461:tf.__operators__.add_4 (TFOpLam (None, 512)          0           fc1_0[0][0]                      
2024-02-15 00:40:30.958505:                                                                 fc1_dim_0[0][0]                  
2024-02-15 00:40:30.958541:__________________________________________________________________________________________________
2024-02-15 00:40:30.958647:fc1_1 (Dense)                   (None, 512)          262656      tf.__operators__.add_4[0][0]     
2024-02-15 00:40:30.958691:__________________________________________________________________________________________________
2024-02-15 00:40:30.958768:tf.__operators__.add_5 (TFOpLam (None, 512)          0           fc1_1[0][0]                      
2024-02-15 00:40:30.958811:                                                                 tf.__operators__.add_4[0][0]     
2024-02-15 00:40:30.958846:__________________________________________________________________________________________________
2024-02-15 00:40:30.958949:fc1_2 (Dense)                   (None, 512)          262656      tf.__operators__.add_5[0][0]     
2024-02-15 00:40:30.958991:__________________________________________________________________________________________________
2024-02-15 00:40:30.959067:tf.__operators__.add_6 (TFOpLam (None, 512)          0           fc1_2[0][0]                      
2024-02-15 00:40:30.959109:                                                                 tf.__operators__.add_5[0][0]     
2024-02-15 00:40:30.959146:__________________________________________________________________________________________________
2024-02-15 00:40:30.959220:dropout_1 (Dropout)             (None, 512)          0           tf.__operators__.add_6[0][0]     
2024-02-15 00:40:30.959259:__________________________________________________________________________________________________
2024-02-15 00:40:30.959361:fc2_0 (Dense)                   (None, 256)          131328      dropout_1[0][0]                  
2024-02-15 00:40:30.959410:__________________________________________________________________________________________________
2024-02-15 00:40:30.959529:fc2_dim_0 (Dense)               (None, 256)          131328      dropout_1[0][0]                  
2024-02-15 00:40:30.959572:__________________________________________________________________________________________________
2024-02-15 00:40:30.959647:tf.__operators__.add_7 (TFOpLam (None, 256)          0           fc2_0[0][0]                      
2024-02-15 00:40:30.959689:                                                                 fc2_dim_0[0][0]                  
2024-02-15 00:40:30.959725:__________________________________________________________________________________________________
2024-02-15 00:40:30.959826:fc2_1 (Dense)                   (None, 256)          65792       tf.__operators__.add_7[0][0]     
2024-02-15 00:40:30.959866:__________________________________________________________________________________________________
2024-02-15 00:40:30.959940:tf.__operators__.add_8 (TFOpLam (None, 256)          0           fc2_1[0][0]                      
2024-02-15 00:40:30.959982:                                                                 tf.__operators__.add_7[0][0]     
2024-02-15 00:40:30.960018:__________________________________________________________________________________________________
2024-02-15 00:40:30.960118:fc2_2 (Dense)                   (None, 256)          65792       tf.__operators__.add_8[0][0]     
2024-02-15 00:40:30.960186:__________________________________________________________________________________________________
2024-02-15 00:40:30.960266:tf.__operators__.add_9 (TFOpLam (None, 256)          0           fc2_2[0][0]                      
2024-02-15 00:40:30.960311:                                                                 tf.__operators__.add_8[0][0]     
2024-02-15 00:40:30.960348:__________________________________________________________________________________________________
2024-02-15 00:40:30.960427:dropout_2 (Dropout)             (None, 256)          0           tf.__operators__.add_9[0][0]     
2024-02-15 00:40:30.960466:__________________________________________________________________________________________________
2024-02-15 00:40:30.960569:fc3_0 (Dense)                   (None, 128)          32896       dropout_2[0][0]                  
2024-02-15 00:40:30.960612:__________________________________________________________________________________________________
2024-02-15 00:40:30.960713:fc3_dim_0 (Dense)               (None, 128)          32896       dropout_2[0][0]                  
2024-02-15 00:40:30.960754:__________________________________________________________________________________________________
2024-02-15 00:40:30.960828:tf.__operators__.add_10 (TFOpLa (None, 128)          0           fc3_0[0][0]                      
2024-02-15 00:40:30.960870:                                                                 fc3_dim_0[0][0]                  
2024-02-15 00:40:30.960906:__________________________________________________________________________________________________
2024-02-15 00:40:30.961005:fc3_1 (Dense)                   (None, 128)          16512       tf.__operators__.add_10[0][0]    
2024-02-15 00:40:30.961046:__________________________________________________________________________________________________
2024-02-15 00:40:30.961119:tf.__operators__.add_11 (TFOpLa (None, 128)          0           fc3_1[0][0]                      
2024-02-15 00:40:30.961161:                                                                 tf.__operators__.add_10[0][0]    
2024-02-15 00:40:30.961197:__________________________________________________________________________________________________
2024-02-15 00:40:30.961296:fc3_2 (Dense)                   (None, 128)          16512       tf.__operators__.add_11[0][0]    
2024-02-15 00:40:30.961336:__________________________________________________________________________________________________
2024-02-15 00:40:30.961427:tf.__operators__.add_12 (TFOpLa (None, 128)          0           fc3_2[0][0]                      
2024-02-15 00:40:30.961490:                                                                 tf.__operators__.add_11[0][0]    
2024-02-15 00:40:30.961528:__________________________________________________________________________________________________
2024-02-15 00:40:30.961610:dropout_3 (Dropout)             (None, 128)          0           tf.__operators__.add_12[0][0]    
2024-02-15 00:40:30.961648:__________________________________________________________________________________________________
2024-02-15 00:40:30.961757:fc4_0 (Dense)                   (None, 64)           8256        dropout_3[0][0]                  
2024-02-15 00:40:30.961800:__________________________________________________________________________________________________
2024-02-15 00:40:30.961902:fc4_dim_0 (Dense)               (None, 64)           8256        dropout_3[0][0]                  
2024-02-15 00:40:30.961943:__________________________________________________________________________________________________
2024-02-15 00:40:30.962017:tf.__operators__.add_13 (TFOpLa (None, 64)           0           fc4_0[0][0]                      
2024-02-15 00:40:30.962060:                                                                 fc4_dim_0[0][0]                  
2024-02-15 00:40:30.962096:__________________________________________________________________________________________________
2024-02-15 00:40:30.962195:fc4_1 (Dense)                   (None, 64)           4160        tf.__operators__.add_13[0][0]    
2024-02-15 00:40:30.962242:__________________________________________________________________________________________________
2024-02-15 00:40:30.962316:tf.__operators__.add_14 (TFOpLa (None, 64)           0           fc4_1[0][0]                      
2024-02-15 00:40:30.962360:                                                                 tf.__operators__.add_13[0][0]    
2024-02-15 00:40:30.962398:__________________________________________________________________________________________________
2024-02-15 00:40:30.962516:fc5_0 (Dense)                   (None, 32)           2080        tf.__operators__.add_14[0][0]    
2024-02-15 00:40:30.962558:__________________________________________________________________________________________________
2024-02-15 00:40:30.962657:fc5_dim_0 (Dense)               (None, 32)           2080        tf.__operators__.add_14[0][0]    
2024-02-15 00:40:30.965864:__________________________________________________________________________________________________
2024-02-15 00:40:30.965959:tf.__operators__.add_15 (TFOpLa (None, 32)           0           fc5_0[0][0]                      
2024-02-15 00:40:30.966003:                                                                 fc5_dim_0[0][0]                  
2024-02-15 00:40:30.966039:__________________________________________________________________________________________________
2024-02-15 00:40:30.966143:fc6 (Dense)                     (None, 1)            33          tf.__operators__.add_15[0][0]    
2024-02-15 00:40:30.966186:==================================================================================================
2024-02-15 00:40:30.966784:Total params: 6,816,545
2024-02-15 00:40:30.966837:Trainable params: 6,816,545
2024-02-15 00:40:30.966876:Non-trainable params: 0
2024-02-15 00:40:30.966910:__________________________________________________________________________________________________
2024-02-15 00:40:30.976623:start training
2024-02-15 00:40:32.288542:2024-02-15 00:40:32.288489: Current epoch: 0, loss: 15.276129722595215, validation loss: 3.0981218814849854
2024-02-15 00:40:32.509059:2024-02-15 00:40:32.509007: Current epoch: 1, loss: 5.2696733474731445, validation loss: 2.89457368850708
2024-02-15 00:40:32.700945:2024-02-15 00:40:32.700910: Current epoch: 2, loss: 3.3158257007598877, validation loss: 2.9086194038391113
2024-02-15 00:40:32.916174:2024-02-15 00:40:32.916128: Current epoch: 3, loss: 3.384507656097412, validation loss: 2.822434425354004
2024-02-15 00:40:33.092224:2024-02-15 00:40:33.092193: Current epoch: 4, loss: 3.047419786453247, validation loss: 3.6547091007232666
2024-02-15 00:40:33.276322:2024-02-15 00:40:33.276286: Current epoch: 5, loss: 3.4540910720825195, validation loss: 3.009488582611084
2024-02-15 00:40:33.479070:2024-02-15 00:40:33.479035: Current epoch: 6, loss: 3.434347629547119, validation loss: 3.1401212215423584
2024-02-15 00:40:33.701840:2024-02-15 00:40:33.701796: Current epoch: 7, loss: 3.1169486045837402, validation loss: 2.7542335987091064
2024-02-15 00:40:33.892305:2024-02-15 00:40:33.892271: Current epoch: 8, loss: 2.9826743602752686, validation loss: 3.4899446964263916
2024-02-15 00:40:34.081867:2024-02-15 00:40:34.081802: Current epoch: 9, loss: 3.3674826622009277, validation loss: 3.591416358947754
2024-02-15 00:40:36.660245:2024-02-15 00:40:36.660214: Current epoch: 10, loss: 2.930190086364746, validation loss: 3.923797607421875
2024-02-15 00:40:36.874066:2024-02-15 00:40:36.874009: Current epoch: 11, loss: 2.934645652770996, validation loss: 2.8430392742156982
2024-02-15 00:40:37.088346:2024-02-15 00:40:37.088286: Current epoch: 12, loss: 3.0898783206939697, validation loss: 2.7558724880218506
2024-02-15 00:40:37.302880:2024-02-15 00:40:37.302845: Current epoch: 13, loss: 2.8466920852661133, validation loss: 3.1083686351776123
2024-02-15 00:40:37.517311:2024-02-15 00:40:37.517245: Current epoch: 14, loss: 2.9685065746307373, validation loss: 2.7578017711639404
2024-02-15 00:40:37.765654:2024-02-15 00:40:37.765606: Current epoch: 15, loss: 2.7507505416870117, validation loss: 2.713273525238037
2024-02-15 00:40:37.967926:2024-02-15 00:40:37.967892: Current epoch: 16, loss: 2.821026086807251, validation loss: 3.078535795211792
2024-02-15 00:40:38.164763:2024-02-15 00:40:38.164727: Current epoch: 17, loss: 3.0652594566345215, validation loss: 2.9443297386169434
2024-02-15 00:40:38.360078:2024-02-15 00:40:38.360044: Current epoch: 18, loss: 2.843414545059204, validation loss: 3.3322548866271973
2024-02-15 00:40:38.584183:2024-02-15 00:40:38.584135: Current epoch: 19, loss: 3.1042673587799072, validation loss: 2.5768320560455322
2024-02-15 00:40:41.079308:2024-02-15 00:40:41.079238: Current epoch: 20, loss: 2.7277371883392334, validation loss: 2.6375458240509033
2024-02-15 00:40:41.323164:2024-02-15 00:40:41.323114: Current epoch: 21, loss: 2.8938379287719727, validation loss: 2.4809422492980957
2024-02-15 00:40:41.518796:2024-02-15 00:40:41.518729: Current epoch: 22, loss: 2.8493270874023438, validation loss: 3.2139229774475098
2024-02-15 00:40:41.729692:2024-02-15 00:40:41.729626: Current epoch: 23, loss: 2.857764720916748, validation loss: 2.7955586910247803
2024-02-15 00:40:41.947210:2024-02-15 00:40:41.947156: Current epoch: 24, loss: 2.942387580871582, validation loss: 2.8598742485046387
2024-02-15 00:40:42.163836:2024-02-15 00:40:42.163804: Current epoch: 25, loss: 2.6366593837738037, validation loss: 2.631127119064331
2024-02-15 00:40:42.366275:2024-02-15 00:40:42.366245: Current epoch: 26, loss: 3.146077871322632, validation loss: 3.6206681728363037
2024-02-15 00:40:42.571113:2024-02-15 00:40:42.571048: Current epoch: 27, loss: 2.8586008548736572, validation loss: 2.653841257095337
2024-02-15 00:40:42.782354:2024-02-15 00:40:42.782317: Current epoch: 28, loss: 2.6532325744628906, validation loss: 2.5774919986724854
2024-02-15 00:40:42.986532:2024-02-15 00:40:42.986500: Current epoch: 29, loss: 2.576655149459839, validation loss: 2.7522387504577637
2024-02-15 00:40:43.203699:2024-02-15 00:40:43.203666: Current epoch: 30, loss: 2.669829845428467, validation loss: 2.6239969730377197
2024-02-15 00:40:43.411996:2024-02-15 00:40:43.411963: Current epoch: 31, loss: 2.489316701889038, validation loss: 2.7735648155212402
2024-02-15 00:40:43.611117:2024-02-15 00:40:43.611049: Current epoch: 32, loss: 2.6002445220947266, validation loss: 3.165149211883545
2024-02-15 00:40:43.823052:2024-02-15 00:40:43.823014: Current epoch: 33, loss: 2.5583345890045166, validation loss: 2.892756700515747
2024-02-15 00:40:44.011958:2024-02-15 00:40:44.011921: Current epoch: 34, loss: 2.6776225566864014, validation loss: 2.604367971420288
2024-02-15 00:40:44.217478:2024-02-15 00:40:44.217383: Current epoch: 35, loss: 2.356553792953491, validation loss: 3.143007755279541
2024-02-15 00:40:44.429228:2024-02-15 00:40:44.429197: Current epoch: 36, loss: 2.6066064834594727, validation loss: 2.854043960571289
2024-02-15 00:40:44.631636:2024-02-15 00:40:44.631570: Current epoch: 37, loss: 2.36100435256958, validation loss: 2.6477510929107666
2024-02-15 00:40:44.844943:2024-02-15 00:40:44.844877: Current epoch: 38, loss: 2.3277852535247803, validation loss: 2.9094457626342773
2024-02-15 00:40:45.057772:2024-02-15 00:40:45.057743: Current epoch: 39, loss: 2.354449510574341, validation loss: 3.152346611022949
2024-02-15 00:40:45.265528:2024-02-15 00:40:45.265460: Current epoch: 40, loss: 2.4036152362823486, validation loss: 2.5442428588867188
2024-02-15 00:40:45.486650:2024-02-15 00:40:45.486615: Current epoch: 41, loss: 2.3641343116760254, validation loss: 2.656036138534546
2024-02-15 00:40:45.694827:2024-02-15 00:40:45.694798: Current epoch: 42, loss: 2.3038017749786377, validation loss: 3.0089704990386963
2024-02-15 00:40:45.906883:2024-02-15 00:40:45.906835: Current epoch: 43, loss: 2.314897298812866, validation loss: 2.7101101875305176
2024-02-15 00:40:46.161193:2024-02-15 00:40:46.161135: Current epoch: 44, loss: 2.209073066711426, validation loss: 2.9764082431793213
2024-02-15 00:40:46.365636:2024-02-15 00:40:46.365598: Current epoch: 45, loss: 2.1126763820648193, validation loss: 3.172680139541626
2024-02-15 00:40:46.568984:2024-02-15 00:40:46.568936: Current epoch: 46, loss: 2.3488051891326904, validation loss: 2.984955072402954
2024-02-15 00:40:46.764967:2024-02-15 00:40:46.764932: Current epoch: 47, loss: 2.628610849380493, validation loss: 3.3159232139587402
2024-02-15 00:40:46.959891:2024-02-15 00:40:46.959856: Current epoch: 48, loss: 2.50750470161438, validation loss: 3.5320632457733154
2024-02-15 00:40:47.169525:2024-02-15 00:40:47.169488: Current epoch: 49, loss: 2.3273766040802, validation loss: 2.6941592693328857
2024-02-15 00:40:47.386933:2024-02-15 00:40:47.386828: Current epoch: 50, loss: 2.1690375804901123, validation loss: 2.646193265914917
2024-02-15 00:40:47.590697:2024-02-15 00:40:47.590667: Current epoch: 51, loss: 2.0151491165161133, validation loss: 2.7194395065307617
2024-02-15 00:40:47.803777:2024-02-15 00:40:47.803691: Current epoch: 52, loss: 2.0874581336975098, validation loss: 2.7106668949127197
2024-02-15 00:40:48.005844:2024-02-15 00:40:48.005807: Current epoch: 53, loss: 2.0525548458099365, validation loss: 2.647108793258667
2024-02-15 00:40:48.199817:2024-02-15 00:40:48.199750: Current epoch: 54, loss: 2.213491439819336, validation loss: 2.9873905181884766
2024-02-15 00:40:48.480460:2024-02-15 00:40:48.480396: Current epoch: 55, loss: 2.0012972354888916, validation loss: 2.4732728004455566
2024-02-15 00:40:48.683318:2024-02-15 00:40:48.683281: Current epoch: 56, loss: 2.0956366062164307, validation loss: 2.525768280029297
2024-02-15 00:40:48.893273:2024-02-15 00:40:48.893236: Current epoch: 57, loss: 2.1063807010650635, validation loss: 2.8755722045898438
2024-02-15 00:40:49.094039:2024-02-15 00:40:49.094006: Current epoch: 58, loss: 1.9711891412734985, validation loss: 2.6327052116394043
2024-02-15 00:40:49.301069:2024-02-15 00:40:49.301036: Current epoch: 59, loss: 2.183690071105957, validation loss: 2.6974987983703613
2024-02-15 00:40:49.527780:2024-02-15 00:40:49.527665: Current epoch: 60, loss: 2.0630452632904053, validation loss: 2.5905025005340576
2024-02-15 00:40:49.740227:2024-02-15 00:40:49.740173: Current epoch: 61, loss: 1.9493118524551392, validation loss: 2.8138160705566406
2024-02-15 00:40:49.950265:2024-02-15 00:40:49.950236: Current epoch: 62, loss: 2.1529769897460938, validation loss: 2.7238636016845703
2024-02-15 00:40:50.161899:2024-02-15 00:40:50.161866: Current epoch: 63, loss: 2.006892204284668, validation loss: 2.8395214080810547
2024-02-15 00:40:50.365527:2024-02-15 00:40:50.365459: Current epoch: 64, loss: 1.8018794059753418, validation loss: 2.6810429096221924
2024-02-15 00:40:50.575803:2024-02-15 00:40:50.575769: Current epoch: 65, loss: 1.907030463218689, validation loss: 2.519231081008911
2024-02-15 00:40:50.787561:2024-02-15 00:40:50.787530: Current epoch: 66, loss: 1.9469853639602661, validation loss: 2.6608495712280273
2024-02-15 00:40:50.991392:2024-02-15 00:40:50.991358: Current epoch: 67, loss: 1.9143391847610474, validation loss: 2.797542095184326
2024-02-15 00:40:51.195033:2024-02-15 00:40:51.194998: Current epoch: 68, loss: 1.7523064613342285, validation loss: 2.9527828693389893
2024-02-15 00:40:51.403863:2024-02-15 00:40:51.403825: Current epoch: 69, loss: 1.7740775346755981, validation loss: 2.7389209270477295
2024-02-15 00:40:51.603350:2024-02-15 00:40:51.603320: Current epoch: 70, loss: 1.7251949310302734, validation loss: 2.5993235111236572
2024-02-15 00:40:51.791738:2024-02-15 00:40:51.791706: Current epoch: 71, loss: 1.7324532270431519, validation loss: 2.764631509780884
2024-02-15 00:40:52.039856:2024-02-15 00:40:52.039763: Current epoch: 72, loss: 1.777258276939392, validation loss: 2.7425777912139893
2024-02-15 00:40:52.255633:2024-02-15 00:40:52.255604: Current epoch: 73, loss: 1.8725107908248901, validation loss: 2.803539514541626
2024-02-15 00:40:52.460150:2024-02-15 00:40:52.460120: Current epoch: 74, loss: 1.9334654808044434, validation loss: 2.658257246017456
2024-02-15 00:40:52.655169:2024-02-15 00:40:52.655133: Current epoch: 75, loss: 1.6152241230010986, validation loss: 3.024630546569824
2024-02-15 00:40:52.870866:2024-02-15 00:40:52.870816: Current epoch: 76, loss: 1.772227168083191, validation loss: 2.6494312286376953
2024-02-15 00:40:53.073381:2024-02-15 00:40:53.073352: Current epoch: 77, loss: 1.6886627674102783, validation loss: 2.7064316272735596
2024-02-15 00:40:53.262815:2024-02-15 00:40:53.262771: Current epoch: 78, loss: 1.692104458808899, validation loss: 2.8934216499328613
2024-02-15 00:40:53.471016:2024-02-15 00:40:53.470983: Current epoch: 79, loss: 1.8194714784622192, validation loss: 2.8268234729766846
2024-02-15 00:40:53.676634:2024-02-15 00:40:53.676599: Current epoch: 80, loss: 1.792631983757019, validation loss: 2.6673288345336914
2024-02-15 00:40:53.887833:2024-02-15 00:40:53.887789: Current epoch: 81, loss: 1.6495498418807983, validation loss: 2.8987371921539307
2024-02-15 00:40:54.085188:2024-02-15 00:40:54.085153: Current epoch: 82, loss: 1.5853924751281738, validation loss: 2.6608259677886963
2024-02-15 00:40:54.292882:2024-02-15 00:40:54.292854: Current epoch: 83, loss: 1.5941901206970215, validation loss: 2.9747040271759033
2024-02-15 00:40:54.497314:2024-02-15 00:40:54.497278: Current epoch: 84, loss: 1.667781114578247, validation loss: 2.7084271907806396
2024-02-15 00:40:54.708707:2024-02-15 00:40:54.708640: Current epoch: 85, loss: 1.3528523445129395, validation loss: 2.6712827682495117
2024-02-15 00:40:54.921978:2024-02-15 00:40:54.921944: Current epoch: 86, loss: 1.4695528745651245, validation loss: 2.8950603008270264
2024-02-15 00:40:55.148811:2024-02-15 00:40:55.148695: Current epoch: 87, loss: 1.5252727270126343, validation loss: 2.9940524101257324
2024-02-15 00:40:55.360632:2024-02-15 00:40:55.360602: Current epoch: 88, loss: 1.700592041015625, validation loss: 2.9926774501800537
2024-02-15 00:40:55.567220:2024-02-15 00:40:55.567184: Current epoch: 89, loss: 1.6637340784072876, validation loss: 2.8402936458587646
2024-02-15 00:40:55.777961:2024-02-15 00:40:55.777922: Current epoch: 90, loss: 1.6109251976013184, validation loss: 2.8083934783935547
2024-02-15 00:40:55.986996:2024-02-15 00:40:55.986968: Current epoch: 91, loss: 1.3802757263183594, validation loss: 2.5628528594970703
2024-02-15 00:40:56.215581:2024-02-15 00:40:56.215537: Current epoch: 92, loss: 1.5357117652893066, validation loss: 2.9026544094085693
2024-02-15 00:40:56.420609:2024-02-15 00:40:56.420581: Current epoch: 93, loss: 1.4135667085647583, validation loss: 2.94826340675354
2024-02-15 00:40:56.626790:2024-02-15 00:40:56.626758: Current epoch: 94, loss: 1.5746409893035889, validation loss: 2.7607781887054443
2024-02-15 00:40:56.835142:2024-02-15 00:40:56.835107: Current epoch: 95, loss: 1.4550837278366089, validation loss: 2.905872106552124
2024-02-15 00:40:57.036149:2024-02-15 00:40:57.036120: Current epoch: 96, loss: 1.5502756834030151, validation loss: 2.6513490676879883
2024-02-15 00:40:57.248668:2024-02-15 00:40:57.248639: Current epoch: 97, loss: 1.5628361701965332, validation loss: 2.6526572704315186
2024-02-15 00:40:57.455368:2024-02-15 00:40:57.455301: Current epoch: 98, loss: 1.3973666429519653, validation loss: 2.9136886596679688
2024-02-15 00:40:57.696058:2024-02-15 00:40:57.695976: Current epoch: 99, loss: 1.443374514579773, validation loss: 2.6780428886413574
2024-02-15 00:40:57.906928:2024-02-15 00:40:57.906861: Current epoch: 100, loss: 1.3257782459259033, validation loss: 2.7397825717926025
2024-02-15 00:40:58.124930:2024-02-15 00:40:58.124900: Current epoch: 101, loss: 1.3311220407485962, validation loss: 2.8551154136657715
2024-02-15 00:40:58.336766:2024-02-15 00:40:58.336698: Current epoch: 102, loss: 1.4150956869125366, validation loss: 3.046902656555176
2024-02-15 00:40:58.553102:2024-02-15 00:40:58.553037: Current epoch: 103, loss: 1.3957781791687012, validation loss: 2.871947765350342
2024-02-15 00:40:58.768448:2024-02-15 00:40:58.768379: Current epoch: 104, loss: 1.4312344789505005, validation loss: 2.5629477500915527
2024-02-15 00:40:58.976222:2024-02-15 00:40:58.976149: Current epoch: 105, loss: 1.4536657333374023, validation loss: 3.0398941040039062
2024-02-15 00:40:59.183996:2024-02-15 00:40:59.183953: Current epoch: 106, loss: 1.444738507270813, validation loss: 2.851384162902832
2024-02-15 00:40:59.408133:2024-02-15 00:40:59.408080: Current epoch: 107, loss: 1.276884913444519, validation loss: 2.6236214637756348
2024-02-15 00:40:59.608766:2024-02-15 00:40:59.608715: Current epoch: 108, loss: 1.2049158811569214, validation loss: 2.8220551013946533
2024-02-15 00:40:59.809498:2024-02-15 00:40:59.809402: Current epoch: 109, loss: 1.215309739112854, validation loss: 2.9082114696502686
2024-02-15 00:41:00.021706:2024-02-15 00:41:00.021668: Current epoch: 110, loss: 1.242489218711853, validation loss: 2.844287395477295
2024-02-15 00:41:00.226879:2024-02-15 00:41:00.226848: Current epoch: 111, loss: 1.3275060653686523, validation loss: 3.107595682144165
2024-02-15 00:41:00.432576:2024-02-15 00:41:00.432499: Current epoch: 112, loss: 1.3527687788009644, validation loss: 2.9026572704315186
2024-02-15 00:41:00.650209:2024-02-15 00:41:00.650171: Current epoch: 113, loss: 1.304728627204895, validation loss: 2.8626043796539307
2024-02-15 00:41:00.867066:2024-02-15 00:41:00.867026: Current epoch: 114, loss: 1.1632779836654663, validation loss: 2.7178714275360107
2024-02-15 00:41:01.075693:2024-02-15 00:41:01.075658: Current epoch: 115, loss: 1.1769566535949707, validation loss: 2.752332925796509
2024-02-15 00:41:01.295541:2024-02-15 00:41:01.295471: Current epoch: 116, loss: 1.078823208808899, validation loss: 2.732879400253296
2024-02-15 00:41:01.505249:2024-02-15 00:41:01.505212: Current epoch: 117, loss: 1.2319679260253906, validation loss: 3.149280309677124
2024-02-15 00:41:01.709000:2024-02-15 00:41:01.708964: Current epoch: 118, loss: 1.0980339050292969, validation loss: 2.8300352096557617
2024-02-15 00:41:01.929335:2024-02-15 00:41:01.929270: Current epoch: 119, loss: 1.1188441514968872, validation loss: 2.9947824478149414
2024-02-15 00:41:02.149985:2024-02-15 00:41:02.149881: Current epoch: 120, loss: 1.2988258600234985, validation loss: 2.807582378387451
2024-02-15 00:41:02.364771:2024-02-15 00:41:02.364699: Current epoch: 121, loss: 1.1484670639038086, validation loss: 2.887777328491211
2024-02-15 00:41:02.580007:2024-02-15 00:41:02.579964: Current epoch: 122, loss: 1.0622737407684326, validation loss: 2.812734603881836
2024-02-15 00:41:02.797931:2024-02-15 00:41:02.797881: Current epoch: 123, loss: 1.0738285779953003, validation loss: 3.0535075664520264
2024-02-15 00:41:03.018769:2024-02-15 00:41:03.018725: Current epoch: 124, loss: 1.0387307405471802, validation loss: 2.7630600929260254
2024-02-15 00:41:03.242669:2024-02-15 00:41:03.242574: Current epoch: 125, loss: 1.0053472518920898, validation loss: 2.8385355472564697
2024-02-15 00:41:03.451169:2024-02-15 00:41:03.451134: Current epoch: 126, loss: 0.9726011753082275, validation loss: 2.6120553016662598
2024-02-15 00:41:03.659811:2024-02-15 00:41:03.659777: Current epoch: 127, loss: 1.0725626945495605, validation loss: 2.8571090698242188
2024-02-15 00:41:03.868150:2024-02-15 00:41:03.868121: Current epoch: 128, loss: 0.9718475341796875, validation loss: 2.867325782775879
2024-02-15 00:41:04.075531:2024-02-15 00:41:04.075461: Current epoch: 129, loss: 1.0408169031143188, validation loss: 2.927598476409912
2024-02-15 00:41:04.294029:2024-02-15 00:41:04.293997: Current epoch: 130, loss: 1.1846436262130737, validation loss: 2.8148515224456787
2024-02-15 00:41:04.504281:2024-02-15 00:41:04.504254: Current epoch: 131, loss: 1.0766186714172363, validation loss: 2.7528483867645264
2024-02-15 00:41:04.713516:2024-02-15 00:41:04.713448: Current epoch: 132, loss: 1.0614858865737915, validation loss: 2.617455244064331
2024-02-15 00:41:04.928116:2024-02-15 00:41:04.928088: Current epoch: 133, loss: 1.0479698181152344, validation loss: 2.9195151329040527
2024-02-15 00:41:05.115727:2024-02-15 00:41:05.115692: Current epoch: 134, loss: 1.090739130973816, validation loss: 2.7036030292510986
2024-02-15 00:41:05.318477:2024-02-15 00:41:05.318442: Current epoch: 135, loss: 0.9731774926185608, validation loss: 2.8360159397125244
2024-02-15 00:41:05.515620:2024-02-15 00:41:05.515554: Current epoch: 136, loss: 1.0383384227752686, validation loss: 2.864976406097412
2024-02-15 00:41:05.726141:2024-02-15 00:41:05.726075: Current epoch: 137, loss: 1.0857446193695068, validation loss: 2.71006441116333
2024-02-15 00:41:05.937168:2024-02-15 00:41:05.937121: Current epoch: 138, loss: 1.065300464630127, validation loss: 3.0405266284942627
2024-02-15 00:41:06.153311:2024-02-15 00:41:06.153278: Current epoch: 139, loss: 1.0302866697311401, validation loss: 2.6363344192504883
2024-02-15 00:41:06.365168:2024-02-15 00:41:06.365134: Current epoch: 140, loss: 0.8528006076812744, validation loss: 2.8406760692596436
2024-02-15 00:41:06.561066:2024-02-15 00:41:06.561039: Current epoch: 141, loss: 0.8148350119590759, validation loss: 2.7515945434570312
2024-02-15 00:41:06.769906:2024-02-15 00:41:06.769840: Current epoch: 142, loss: 0.9325650930404663, validation loss: 2.799424886703491
2024-02-15 00:41:06.982984:2024-02-15 00:41:06.982957: Current epoch: 143, loss: 1.1537121534347534, validation loss: 2.56011700630188
2024-02-15 00:41:07.197669:2024-02-15 00:41:07.197609: Current epoch: 144, loss: 0.989052951335907, validation loss: 3.036026954650879
2024-02-15 00:41:07.407516:2024-02-15 00:41:07.407488: Current epoch: 145, loss: 1.0034031867980957, validation loss: 2.922959327697754
2024-02-15 00:41:07.609527:2024-02-15 00:41:07.609498: Current epoch: 146, loss: 1.0240705013275146, validation loss: 2.826960802078247
2024-02-15 00:41:07.815455:2024-02-15 00:41:07.815403: Current epoch: 147, loss: 0.9219789505004883, validation loss: 2.894181489944458
2024-02-15 00:41:08.018995:2024-02-15 00:41:08.018929: Current epoch: 148, loss: 0.8256723284721375, validation loss: 2.7479593753814697
2024-02-15 00:41:08.228515:2024-02-15 00:41:08.228461: Current epoch: 149, loss: 0.9939689636230469, validation loss: 2.6088571548461914
2024-02-15 00:41:08.424696:2024-02-15 00:41:08.424668: Current epoch: 150, loss: 0.8538362979888916, validation loss: 3.022935390472412
2024-02-15 00:41:08.632580:2024-02-15 00:41:08.632536: Current epoch: 151, loss: 0.8820693492889404, validation loss: 2.7744827270507812
2024-02-15 00:41:08.844773:2024-02-15 00:41:08.844717: Current epoch: 152, loss: 0.9399608969688416, validation loss: 2.7607052326202393
2024-02-15 00:41:09.046778:2024-02-15 00:41:09.046712: Current epoch: 153, loss: 0.7982541918754578, validation loss: 2.6792986392974854
2024-02-15 00:41:09.268085:2024-02-15 00:41:09.268017: Current epoch: 154, loss: 0.9403465390205383, validation loss: 2.6501739025115967
2024-02-15 00:41:09.551335:2024-02-15 00:41:09.551283: Current epoch: 155, loss: 0.9297165274620056, validation loss: 2.7025146484375
2024-02-15 00:41:09.618579:the test error is  [2.547187089920044, 2.547187089920044]
2024-02-15 00:41:12.102757:saved model to sample/bert-base-uncased_robo_prop_Tc_supercon
2024-02-15 00:41:12.313061:done